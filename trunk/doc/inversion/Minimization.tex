
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copyright (c) 2003-2012 by University of Queensland
% http://www.uq.edu.au
%
% Primary Business: Queensland, Australia
% Licensed under the Open Software License version 3.0
% http://www.opensource.org/licenses/osl-3.0.php
%
% Development until 2012 by Earth Systems Science Computational Center (ESSCC)
% Development since 2012 by School of Earth Sciences
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Minimization Algorithms}\label{chapter:ref:Minimization}
THIS NEEDS REVISION


Let $f(\mat{x}), \mat{x} \in \mathbb{R}^n$ be a smooth nonlinear function.
There are a number of algorithms that try to find a minimum of $f(\mat{x})$
which have different requirements on $f$ in order to be successful.
We assume that $n$ is large and computing the Hessian matrix
$\nabla^2 f(\mat{x})$ is not feasible.
The algorithms that are most widely used in the literature for this type of
optimization problem is the 
%the \emph{Nonlinear Conjugate Gradient} (\emph{NLCG}) 
the limited-memory Broyden-Fletcher-Goldfarb-Shanno (\emph{L-BFGS}),
which is a \emph{quasi-Newton} method.



% 
%\section{The Nonlinear Conjugate Gradient method}\label{sec:NLCG}
%The steps for NLCG are
%\begin{enumerate}
%    \item Given an initial guess $\mat{x}_0$, compute the steepest descent
%        direction using the gradient: $\mat{p}_0=-\nabla f(\mat{x}_0)$
%    \item Perform an inexact \emph{line search} (see Section~\ref{sec:LineSearch}) to obtain the step
%        length $\alpha_i$ which loosely minimizes $f(\mat{x}_i+\alpha_i \mat{d}_i)$
%    \item Update the position: $\mat{x}_{i+1}=\mat{x}_i+\alpha_i \mat{d}_i$
%    \item Update the conjugate direction: $\mat{d}_{i+1}=-\nabla f(\mat{x}_{i+1})+\beta_{i+1} \mat{d}_i$\label{CGupdate}
%\end{enumerate}

%\flushleft There are many choices for the CG update parameter $\beta_{i+1}$ in step
%\ref{CGupdate} above, see \cite{Hager2006} for a discussion. An efficient
%choice, proposed by Polak and Ribi\`{e}re, is:
%\begin{equation*}
%    \beta_{i+1}=\text{max}\left\{\frac{\nabla f^T(\mat{x}_{i+1})(\nabla f(\mat{x}_{i+1})-\nabla f(\mat{x}_i))}{\| \nabla f(\mat{x}_i)\|}, 0\right\}
%\end{equation*}

\begin{methoddesc}[CostFunction]{resetCounters}{}
    resets all statistical counters.
\end{methoddesc}
%
\begin{methoddesc}[CostFunction]{getInner}{f0, f1}
    returns the inner product of \member{f0} and \member{f1}.
\end{methoddesc}
%
\begin{methoddesc}[CostFunction]{getValue}{x, *args}
    returns the value $f(x)$ using precalculated values for $x$.
\end{methoddesc}
%
\begin{methoddesc}[CostFunction]{getGradient}{x, *args}
    returns the gradient of $f$ at $x$ using precalculated values for $x$.
\end{methoddesc}
%
\begin{methoddesc}[CostFunction]{getDirectionalDerivative}{x, d, *args}
    returns \texttt{inner(grad(f(x)), d)} using precalculated values for $x$.
\end{methoddesc}
%
\begin{methoddesc}[CostFunction]{getArguments}{x}
    returns precalculated values that are shared in the calculation of $f(x)$
    and \texttt{grad(f(x))}.
\end{methoddesc}

%\section{Limited-Memory BFGS Method}\label{sec:LBFGS}
The L-BFGS algorithm is as follows:

\begin{algorithm}
    \mbox{L-BFGS}
    \begin{program}
        \BEGIN
        \text{Input: initial guess $x_0$ and integer $m$};
        \text{Allocate $m$ vector pairs \{$s_i$, $y_i$\}};
        k \leftarrow 0;
        H_0 \leftarrow I;
        \WHILE \NOT\text{converged} \DO
        p_k \leftarrow |twoLoop|(H_k, \{s, y\});
        \alpha_k \leftarrow |lineSearch|(f, x_k, p_k); \rcomment{See Section~\ref{sec:LineSearch}}
        x_{k+1} \leftarrow x_k+\alpha_k p_k;
        \IF k>m
        \THEN
            |Discard the vector pair | \{s_{k-m}, y_{k-m}\} | from storage|;
        \FI
        s_k \leftarrow x_{k+1}-x_k;
        y_k \leftarrow \nabla f_{k+1}-\nabla f_k;
        \gamma_k \leftarrow \frac{[s_k,y_k]}{[y_k, y_k]_0};
        H_k \leftarrow \gamma_k I
        k \leftarrow k+1;
        \OD
        \WHERE
        \FUNCT |twoLoop|(H_k, \{s, y\}) \BODY
            \EXP q \leftarrow \nabla f_k;
            \FOR i=k-1, k-2, \ldots, k-m \DO
            \rho_i \leftarrow \frac{1}{y^T_i s_i};
            \alpha_i \leftarrow \rho_i s^T_i q;
            q \leftarrow q-\alpha_i y_i;
            \OD
            r \leftarrow H_k q;
            \FOR i = k-m, k-m+1, \ldots, k-1 \DO
            \beta \leftarrow \rho_i y^T_i r;
            r \leftarrow r + s_i (\alpha_i - \beta)
            \OD
            r \ENDEXP \ENDFUNCT
        \END
    \end{program}
\end{algorithm}

\section{Line Search}\label{sec:LineSearch}
See \cite{Nocedal2006}, \cite{MoreThuente1992}

\begin{algorithm}
    \mbox{Line Search that satisfies strong Wolfe conditions}
    \begin{program}
        \BEGIN
        \text{Input: $\alpha_{max}>0, 0<c_1<c_2<1$};
        i \leftarrow 1;
        \WHILE 1 \DO
        \IF \phi(\alpha_i) > \phi(0)+c_1 \alpha_i \phi'(0) \OR
            (\phi(\alpha_i) \ge \phi(\alpha_{i-1}) \AND i>1)
        \THEN
            \alpha_* \leftarrow |zoom|(\alpha_{i-1}, \alpha_i);
            |break|;
        \FI
        \IF \|\phi'(\alpha_i)\| \le -c_2\phi'(0)
        \THEN
            \alpha_* \leftarrow \alpha_i;
            |break|;
        \FI
        \IF \phi'(\alpha_i) \ge 0
        \THEN
            \alpha_* \leftarrow |zoom|(\alpha_i, \alpha_{i-1});
            |break|;
        \FI
        \alpha_{i+1} = 2\alpha_i;
        i \leftarrow i+1;
        \OD
        \WHERE
        \FUNCT |zoom|(\alpha_{lo}, \alpha_{hi}) \BODY
            \WHILE 1 \DO
                \EXP \alpha_j \leftarrow \alpha_{lo}+\frac{\alpha_{hi}-\alpha_{lo}}{2};
                \IF \phi(\alpha_j) > \phi(0)+c_1 \alpha_j\phi'(0) \OR %
                    \phi(\alpha_j) \ge \phi(\alpha_{lo})
                \THEN
                    \alpha_{hi} \leftarrow \alpha_j;
                \ELSE
                    \IF \|\phi'(\alpha_j)\| \le -c_2\phi'(0)
                    \THEN
                        \alpha_* \leftarrow \alpha_j;
                        |break|;
                    \FI
                    \IF \phi'(\alpha_j)(\alpha_{hi}-\alpha_{lo}) \ge 0
                    \THEN
                        \alpha_{hi} \leftarrow \alpha_{lo};
                    \FI
                    \alpha_{lo}=\alpha_j;
                \FI
            \OD
            \ENDEXP \ENDFUNCT
        \END
    \end{program}
\end{algorithm}

