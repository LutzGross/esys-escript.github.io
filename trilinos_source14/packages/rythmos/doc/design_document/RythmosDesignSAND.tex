\documentclass[pdf,ps2pdf,11pt]{SANDreport}
\usepackage{pslatex}

%Local stuff
\usepackage{graphicx}
\usepackage{latexsym}
\input{rab_commands}
\newtheorem{theorem}{Theorem}

% If you want to relax some of the SAND98-0730 requirements, use the "relax"
% option. It adds spaces and boldface in the table of contents, and does not
% force the page layout sizes.
% e.g. \documentclass[relax,12pt]{SANDreport}
%
% You can also use the "strict" option, which applies even more of the
% SAND98-0730 guidelines. It gets rid of section numbers which are often
% useful; e.g. \documentclass[strict]{SANDreport}

% ---------------------------------------------------------------------------- %
%
% Set the title, author, and date
%

\title{
Rythmos: A set of abstract and concrete C++ classes for the solution and
analysis of DAEs and ODEs
}
\author{
Roscoe A. Bartlett \\ Optimization/Uncertainty Estim \\ \\
Todd Coffey \\ Computational Math/Algorithms \\ \\
... \\ \\
Sandia National Laboratories\footnote{
Sandia is a multiprogram laboratory operated by Sandia Corporation, a
Lockheed-Martin Company, for the United States Department of Energy
under Contract DE-AC04-94AL85000.}, Albuquerque NM 87185 USA, \\
}
\date{}

% ---------------------------------------------------------------------------- %
% Set some things we need for SAND reports. These are mandatory
%
\SANDnum{SAND2005-xxxx}
\SANDprintDate{April 26, 2005}
\SANDauthor{
Roscoe A. Bartlett \\ Optimization/Uncertainty Estim \\ \\
Todd Coffey \\ Computational Math/Algorithms \\ \\
... \\ \\
}

% ---------------------------------------------------------------------------- %
% The following definitions are optional. The values shown are the default
% ones provided by SANDreport.cls
%
%\SANDreleaseType{Unlimited Release}
\SANDreleaseType{Not approved for release outside Sandia}

% ---------------------------------------------------------------------------- %
% The following definition does not have a default value and will not
% print anything, if not defined
%
%\SANDsupersed{SAND1901-0001}{January 1901}

% ---------------------------------------------------------------------------- %
%
% Start the document
%
\begin{document}
\raggedright

\maketitle

% ------------------------------------------------------------------------ %
% An Abstract is required for SAND reports
%

%
\begin{abstract}
%
Blah blah blah ...
%
\end{abstract}
%

% ------------------------------------------------------------------------ %
% An Acknowledgment section is optional but important, if someone made
% contributions or helped beyond the normal part of a work assignment.
% Use \section* since we don't want it in the table of context
%
\clearpage
\section*{Acknowledgment}
The authors would like to thank ...

The format of this report is based on information found
in~\cite{Sand98-0730}.

% ------------------------------------------------------------------------ %
% The table of contents and list of figures and tables
% Comment out \listoffigures and \listoftables if there are no
% figures or tables. Make sure this starts on an odd numbered page
%
\clearpage
\tableofcontents
\listoffigures
%\listoftables

% ---------------------------------------------------------------------- %
% An optional preface or Foreword
%\clearpage
%\section{Preface}
%Although muggles usually have only limited experience with
%magic, and many even dispute its existence, it is worthwhile
%to be open minded and explore the possibilities.

% ---------------------------------------------------------------------- %
% An optional executive summary
%\clearpage
%\section{Summary}
%Once a certain level of mistrust and scepticism has
%been overcome, magic finds many uses in todays science
%and engineering. In this report we explain some of the
%fundamental spells and instruments of magic and wizardry. We
%then conclude with a few examples on how they can be used
%in daily activities at national Laboratories.

% ---------------------------------------------------------------------- %
% An optional glossary. We don't want it to be numbered
%\clearpage
%\section*{Nomenclature}
%\addcontentsline{toc}{section}{Nomenclature}
%\begin{itemize}
%\item[alohomora]
%spell to open locked doors and containers
%\end{itemize}

% ---------------------------------------------------------------------- %
% This is where the body of the report begins; usually with an Introduction
%
\setcounter{secnumdepth}{3}
\SANDmain % Start the main part of the report

\section{Introduction}

Here we design and describe a set of C++ interfaces and concrete
implementations for the solution of a broad class of transient ordinary
differential equations (ODEs) and differential algebraic equations (DAEs) in a
consistent manner.

\section{Requirements}

Here we describe requirements in two general categories: strong and weak
(i.e.\ wants).

\subsection{Strong requirements}

Here we list requirements that are strongly needed or desired in the time
integrator software.

\begin{enumerate}

{}\item Provide a single interface that applications can implement that allows
for the development of many different types of popular explicit and implicit
initial-value ODE and DAE solvers.

{}\item Provide a single interface where an application can seamlessly select
from one of a number of available time integration methods.

{}\item Exploit as much as possible a common set of software interfaces and
utilities to combine support for ODEs and DAEs together.  Minimize specialized
code and algorithms that apply only to special classes of ODEs and DAEs.

{}\item Provide an automated support framework for direct and adjoint
sensitivity methods for ODEs and DAEs.

{}\item Present an DAE/ODE interface for applications to implement that
upwardly supports the computation of (discrete) direct and adjoint
sensitivities and other types of information.

{}\item For direct sensitivity computations for spatially discretized PDEs,
allow the use of a continuous sensitivity equation instead of a discrete
approximation if desired.

{}\item For adjoint sensitivity computations for spatially discretized PDEs,
allow the use of a continuous adjoint instead of the discrete adjoint if
desired.

{}\item Provide support for general checkpointing of a time integration method
and a uniform way to deal with different time integration methods.

{}\item Require checkpointing of data to result in being able to go back and
continue a computation exactly as if the method would have continued on as
usual.

{}\item Use the basic checkpointing capability to implement checkpointing
methods for nonlinear adjoint sensitivity computations in a way that is
independent of specific applications.

{}\item Allow applications detailed control over the time stepping algorithms
to support breakpoints (e.g.\ for discontinuities) and pauses (e.g.\ for code coupling). 

{}\item Allow basic time stepping algorithm building blocks to be composed
together to build various types of operator splitting methods for complex
physics and multi-physics coupling.

{}\item Provide numerical algorithms to solve non-stiff and stiff ODEs (and
implicit ODEs)

{}\item Provide numerical algorithms to solve fully-implicit and semi-explicit
index-1 DAEs

{}\item Support both application provided and general methods for
preconditioning.

{}\item Provide concrete implementations for the following transient integration methods:
  \begin{enumerate}
  {}\item implicit/explicit BDF (multi-step) \\
        including variable-order variable-step as per CVODE/IDA \cite{CVODE,IDA}.
  {}\item generalized-alpha \cite{ChungHulbert}.
  {}\item explicit Runge Kutta (one-step) \cite{ERKMethods}.
  {}\item Trapezoid \cite{Trapezoid}.
  {}\item Pseudo-Transient Continuation \cite{PTC,PTCDAE}.
  \end{enumerate}

{}\item Operate efficiently on massively parallel distributed memory
computers such as ``Red Storm".

{}\item Document transient integration algorithms with references.

{}\item Document and allow access to internal algorithm coefficients
(so-called ``magic-numbers").

{}\item Provide a well-defined and well-documented way to add and modify
existing transient integration algorithms without requiring modification of
existing source code (this is critical for maintainability).

{}\item Provide an interface to estimating (local/global)
accuracy/error in the solution.

{}\item Allow users to give an upper bound for (local/global) accuracy/error
in the solution that time integrators must satisfy.  This is needed for
certain types of error-controlling optimization algorithms.

{}\item Be compatible with automatic differentiation (AD).

{}\item Allow recording of adaptive time steps so that same steps can be used
in perturbed solutions.  This feature allows for better performance of line
searches and other computations that have a smoothness requirement that may be
violated if different adaptive time steps are taken resulting in a
discontinuity.

\end{enumerate}

\subsection{Weak requirements (wants)}

Here we list requirements that have a much lower priority and may be
considered optional.

\begin{enumerate}

{}\item Support the following transient integration methods:
  \begin{enumerate}
  {}\item implicit Runge Kutta (one-step) \cite{IRKMethods}.
  {}\item general predictor-corrector methods.
  {}\item discontinuous-Galerkin in time \cite{DGTime}.
  {}\item continuous-Gelerkin in time \cite{CGTime}
  \end{enumerate}

{}\item Be compatible with space/time discretizations.

{}\item As much as possible, build step-size and order adaptivity strategies
independent from specific time integration methods.

{}\item Allow for use of CVODE, CVODES, IDA and IDAS at some level [???].

\end{enumerate}

\section{Mathematical Formulation of DAEs/ODEs for Basic Time Steppers}
\label{rythmos:scn:mathformulation}

Here we describe the basic mathematical form of a general nonlinear DAE (or
ODE) for the purpose of presenting it to a forward time integrator.  At the
most general level of abstraction, we will consider the solution of fully
implicit DAEs of the form
%
\begin{eqnarray}
f(\dot{x},x,t) & = & 0, \; \mbox{for} \; t \in [t_0, t_f], \label{rythmos:eqn:dae} \\
x(t_0) & = & x_0 \label{rythmos:eqn:dae:ic}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where\hspace{5ex}\= \\
\>	$x\:\in\:\mathcal{X}$ is the vector of differential state variables, \\
\>	$\dot{x} = d(x)/d(t)\:\in\:\mathcal{X}$ is the vector of temporal derivatives of $x$, \\
\>	$t, t_0, t_f\:\in\:\RE$ are the current, initial, and the final times respectively, \\
\>	$f(\dot{x},x,t)\:\in\:\mathcal{X}^2\times\RE\rightarrow\mathcal{F}$ defines the DAE vector function, \\
\>	$\mathcal{X} \:\subseteq\:\RE^{n_x}$ is the vector space of the state variables $x$, and \\
\>	$\mathcal{F} \:\subseteq\:\RE^{n_x}$ is the vector space of the output of the DAE function $f(\ldots)$.
\end{tabbing}

Here we have been careful to define the Hilbert vector spaces $\mathcal{X}$
and $\mathcal{F}$ for the involved vectors and vector functions.  The
relevance of defining these vector spaces is that they come equipped with a
definition of the space's scalar product (i.e.\ $<u,v>_{\mathcal{X}}$ for
$u,v\in\mathcal{X}$) which should be considered and used when designing the
numerical algorithms.

The above general DAE can be specialized to more specific types of problems
based on the nature of Jacobian matrices
$\jac{f}{\dot{x}}\in\mathcal{F}|\mathcal{X}$ and
$\jac{f}{x}\in\mathcal{F}|\mathcal{X}$.  Here we use the notation
$\mathcal{F}|\mathcal{X}$ to define a linear operator space that maps vectors
from the vector space $\mathcal{X}$ to the vector space $\mathcal{F}$.  Note
that the adjoint of such linear operators are defined in terms of these vector
spaces (i.e.\ $<A u,v>_{\mathcal{F}} = <A^T v,u>_{\mathcal{X}}$ where
$A\in\mathcal{F}|\mathcal{X}$ and $A^T$ denotes the adjoint operator for $A$).
Here we assume that these first derivatives exist for the specific intervals
of $t\in[t_0,t_f]$ of which such an time integrator algorithm will be directly
applied the the problem.

The precise class of the problem is primarily determined by the nature of the
Jacobian $\jac{f}{\dot{x}}$:
%
\begin{itemize}
%
{}\item $\jac{f}{\dot{x}} = I {}\in {}\mathcal{F}|\mathcal{X}$ yields an explicit ODE
%
{}\item $\jac{f}{\dot{x}}$ full rank yields an implicit ODE
%
{}\item $\jac{f}{\dot{x}}$ rank deficient yields a general
DAE.
\end{itemize}
%
In addition, the ODE/DAE may be linear or nonlinear and DAEs are classified by
their index \cite{BCP}.  It is expected that a DAE will be able to tell a 
integrator what type of problem it is (i.e.\ explicit ODE, implicit ODE,
general DAE) and which, if any, of the variables are linear in the problem.
This type of information can be exploited in a time integration algorithm.

{}\textbf{Ross: We need notation for taking advantage of which ``variables are
linear in the problem."}

Another formulation we will consider is the semi-explicit DAE formulation:
\begin{equation}
\label{rythmos:eqn:dae:semiexplicit} 
\begin{array}{rcl}
\dot{y} & = & f(y,z,t) \; \mbox{for} \; t \in [t_0, t_f], \\
0       & = & g(y,z,t) \; \mbox{where} \; x = [y, z], \\
x(t_0)  & = & x_0.
\end{array}
\end{equation}

For each transient computation, the formulation will be cast into the general
form in (\ref{rythmos:eqn:dae})--(\ref{rythmos:eqn:dae:ic}) to demonstrate how
this general formulation can be exploited in many different contexts.

\section{Mathematics of Basic Time Integration Strategies}

In this section we show how time steps are formulated and computed for a
variety of popular explicit and implicit time integration methods.  In general,
all of these time integration methods are based on some basic smoothness
assumptions of the underlying DAE/ODE but the details of the theoretical
background for these methods will not be covered here.

We first consider explicit methods followed by implicit methods.

\subsection{Formulation for Explicit Time Steppers for ODEs}

Explicit integration methods are primarily only attractive for non-stiff
explicit and implicit ODEs but some classes of DAEs can be considered as well
(i.e.\ by nonlinearly eliminating the algebraic variables from the
semi-explicit DAE formulation (\ref{rythmos:eqn:dae:semiexplicit})
\cite{BCP}).  For this discussion, we will also assume that the DAE has been
written in the explicit ODE form (i.e.\ $\jac{f}{\dot{x}} = I$).  Note that
implicit ODEs can always be written as explicit ODEs by multiplying the
implicit ODE from the left with $(\jac{f}{\dot{x}})^{-1}$ as:
%
\begin{eqnarray*}
f(\dot{x},x,t) & = & 0 \\
& \Rightarrow \\
\Jac{f}{\dot{x}} \dot{x} + \hat{f}(x,t) & = & 0 \\
& \Rightarrow \\
\left( \Jac{f}{\dot{x}} \right)^{-1}
\left( \Jac{f}{\dot{x}} \dot{x} + \hat{f}(x,t) \right) & = & 0 \\
& \Rightarrow \\
\dot{x} & = & -\left( \Jac{f}{\dot{x}} \right)^{-1} \hat{f}(x,t) \\
& = & \bar{f}(x,t) \\
\end{eqnarray*}
%
where ${}\dot{x} = \bar{f}(x,t)$ is the new explicit form of the ODE that is
considered by the explicit time integration strategies below.

The above transformation of course requires that the matrix
$(\jac{f}{\dot{x}})^{-1}$ be fairly easy to invert.

Below, we describe some popular explicit integration methods for ODEs.

\subsubsection{Explicit Runge-Kutta methods}

Strong Stability Preserving (SSP) Methods \cite{GottliebShuTadmor,GottliebGottlieb}

\subsubsection{Explicit multi-step methods}

\subsection{Formulation for Implicit Time Steppers for ODEs and DAEs}
\label{rythmos:sec:implicit-time-steppers}

Here we consider several different classes of implicit time stepping methods.
For each class of method we show the set of general nonlinear equations that
defines a single time step and then show how a linearized form of the
equations may be formed to be solved by a Newton-type nonlinear equation
solver.

In particular, for each method, we will show how to define a set of nonlinear
equations of the form
%
\begin{equation}
r(z) = 0
\label{rythmos:eqn:r}
\end{equation}
%
such that when solved will define an implicit time step from $t_k$ to
$t_{k+1}$, where $\Delta t = t_{k+1} - t_k$ denotes the time-step.  In
addition, for each method, we will show how to use the DAE residual evaluation
$(\dot{x},x,t) {}\rightarrow f$ to define the nonlinear time step equation
(\ref{rythmos:eqn:r}).  At the highest level, the time step method only
requires very general convergence criteria for the time step equation
(\ref{rythmos:eqn:r}) and therefore great flexibility is allowed in how the
time step equation is solved.  In general, the system in (\ref{rythmos:eqn:r})
must be solved such that $||x_{k+1} - x^*(t_{k+1})|| < \eta$, where
$x_{k+1}\in\mathcal{X}$ is the computed step for the state,
$x^*(t_{k+1})\in\mathcal{X}$ is the exact state solution at $t_{k+1}$, and
$\eta$ is the maximum allowable local truncation error defined by the user.

Even through the time step equation can be solved by a variety of means, a
large class of DAEs can also potentially provide support for a general
Newton-type method for solving these equations and can therefore leverage
general software for solving such problems (e.g.\ NOX).  The foundation of
Newton-like methods is the ability to solve linear systems similar to the
Newton system
%
\begin{equation}
\Jac{r}{z} \Delta z = - r(z_l)
\end{equation}
%
where $z_l$ is the current candidate solution of the nonlinear equations
(which also defines the point where $\jac{r}{z}$ is evaluated) and $\Delta z =
r_{l+1} - r_l$ is the update direction.  Line-search Newton methods then
define an update to the solution along the direction $\Delta z$.  The
essential functionality needed to perform a Newton-like method are the the
abilities to evaluate the nonlinear residual $z {}\rightarrow r$ and to
(approximately) solve linear systems involving the Jacobian matrix
$\jac{r}{z}$.  For each type of implicit time integration method, we will
show, if possible, how to perform solves with $\jac{r}{z}$ by performing
solves with the matrix
%
\begin{equation}
M = \alpha \Jac{f}{\dot{x}} + \beta \Jac{f}{x},
\label{rythmos:eqn:M}
\end{equation}
%
evaluated at points $(\dot{x},x,t)$ selected by the time integration method
and where $\alpha\in\RE$ and $\beta\in\RE$ is some constants also defined by
the time integration method.  Note that the matrix $M$ above in
(\ref{rythmos:eqn:M_bdf}) may not necessarily exactly represent $\jac{r}{z}$
and $z$ and $r$ may not simply lie in the vector spaces $\mathcal{X}$ and
$\mathcal{F}$ respectively; but in many cases, they will.

\subsubsection{Implicit multi-step BDF methods}

The first class of methods that we will consider are so called multi-step
backward difference formula (BDF) methods.  In these methods, an $p$-step
approximation for the time derivative $\dot{x}$ for the new time point
$t_{k+1}$ is formed based on a linear combination of state values $y_{k+1-j}$
for next, current and previous time values $t_{k+1-j}$, for $j = 0 {}\ldots p$
and the form of the approximation is
%
\begin{equation}
\dot{x}_{k+1} = \frac{1}{\Delta t} \sum_{j=0}^{p} \gamma_j \: x_{k+1-j}
\label{rythmos:eqn:bdf_x_dot}
\end{equation}
%
where $\Delta t = t_{k+1} - t_k$ and $\gamma_j$, for $j=0 {}\ldots p$, are the
BDF method coefficients \cite{AscherPetzold}.

The nonlinear time step equation to advance the solution from $t_k$ to
$t_{k+1}$ is then formed by substituting $\dot{x} = \dot{x}_{k+1}$ in
(\ref{rythmos:eqn:bdf_x_dot}), $x = x_{k+1}$ and $t = t_{k+1}$ into
(\ref{rythmos:eqn:dae}) to obtain
%
\begin{equation}
f\left( \frac{1}{\Delta t} \left[ \gamma_0 x_{k+1} + \sum_{j=1}^{p} \gamma_j \: x_{k+1-j} \right],x_{k+1},t_{k+1}\right) = 0.
\label{rythmos:eqn:bdf_dae_ne}
\end{equation}
%
One can immediately identify the BDF time step equations
(\ref{rythmos:eqn:bdf_dae_ne}) with the general form of the time step
equations (\ref{rythmos:eqn:r}) and with unknown solution variables $z =
x_{k+1}$.  All of the other state variables $x_{k+1-j}$, for $j = 1 {}\ldots
p$, are given.

Note that the first-order BDF method with $p=1$, $\gamma_0 = 1$ and $\gamma_1 =
-1$ is simply the standard backward Euler time integration method \cite{AscherPetzold}.

When considering a general Newton-like method for solving
(\ref{rythmos:eqn:bdf_dae_ne}), note that the Newton Jacobian of these
equations is
%
\begin{equation}
\Jac{r}{z}
= \frac{\gamma_0}{\Delta t} \Jac{f}{\dot{x}} + \Jac{f}{x},
\label{rythmos:eqn:M_bdf}
\end{equation}
%
which is evaluated at the point $\dot{x}$ in (\ref{rythmos:eqn:bdf_x_dot}), $x
= x_{k+1}$ and $t = t_{k+1}$.  One can immediately identify
(\ref{rythmos:eqn:M_bdf}) with the general form of the matrix $M$ in
(\ref{rythmos:eqn:M}) where $\alpha = {}\gamma_0 / \Delta t$ and $\beta = 1$.
Note that the Jacobian (\ref{rythmos:eqn:M_bdf}) is the exact Jacobian for the
nonlinear time step equations; this will not be true for some of the other
methods.

\subsubsection{Generalized Theta methods}

The next fairly straightforward class of methods are the so called $\theta$
(Theta) methods \cite{HairerWanner}.  Theta methods define a time step
equation by requiring enforcement of the DAE equation at an intermediate time
in the range $t_{\theta} {}\in [t_k,t_{k+1}]$ where $t_{\theta} = t_k + \theta
( t_{k+1} - t_k )$ and $\theta {}\in [0,1]$.  Then, $\theta$-averaged values
for $x$ and $t$ are then used to form the implicit time step equation
%
\begin{equation}
f\left( \frac{x_{k+1} - x_{k}}{\Delta t},x_k + \theta ( x_{k+1} - x_{k} ), t_k + \theta \Delta t \right) = 0
\label{rythmos:eqn:theta_dae_ne}
\end{equation}
%
which is then solved for the unknown state $x_{k+1}$.  The values of $\theta =
0$, $\myonehalf$ and $1$ give the well known forward Euler, midpoint
rule, and backward Euler methods \cite{HairerWanner}.

The Jacobian for the nonlinear system in (\ref{rythmos:eqn:theta_dae_ne}), with
$z = x_{k+1}$, is
%
\begin{equation}
\Jac{r}{z}
= \frac{1}{\Delta t} \Jac{f}{\dot{x}} + \theta \Jac{f}{x},
\label{rythmos:eqn:M_theta}
\end{equation}
%
which is of general form of the matrix $M$ in (\ref{rythmos:eqn:M}) where
$\alpha = 1 / \Delta t$ and $\beta = \theta$.

\subsubsection{Trapezoidal Methods}

The next class of methods that we consider are Trapezoidal methods.
Trapezoidal methods are essentially equivalent to the trapezoidal rule for
numerical quadrature (i.e.\ integrating under a curve) and takes the form of
the averaging of the DAE at $t_k$ and $t_{k+1}$ as
%
\begin{equation}
\frac{1}{2} \left[
f\left( \frac{x_{k+1} - x_{k}}{\Delta t}, x_{k+1} , t_{k+1} \right)
+ f\left( \frac{x_{k+1} - x_{k}}{\Delta t}, x_k , t_k \right)
\right]
 = 0
\label{rythmos:eqn:trap_dae_ne}
\end{equation}
%
which defines a set of nonlinear equations that are solved for the unknown
state variables $x_{k+1}$.  The straightforward Newton Jacobian for the
equations in (\ref{rythmos:eqn:trap_dae_ne}) is
%
\begin{equation}
\Jac{r}{z}
= \frac{1}{\Delta t} \left[
\left( \Jac{f}{\dot{x}} \right)_{k+1}
+ \left( \Jac{f}{\dot{x}} \right)_{k}
\right]
+ \left( \Jac{f}{x} \right)_{k+1},
\label{rythmos:eqn:drdz_trap}
\end{equation}
%
where we use the shorthand notation of $(\ldots)_{k+1}$ and $(\ldots)_k$ to
mean that an object is evaluated at the points $(\dot{x},x,t) =
((x_{k+1}-x_k)/\Delta t, x_{k+1}, t_{k+1})$ and $(\dot{x},x,t) =
((x_{k+1}-x_k)/\Delta t, x_k, t_k)$ respectively.  Note that the form of the
Jacobian in (\ref{rythmos:eqn:drdz_trap}) does not match the form of
(\ref{rythmos:eqn:M}) that we are trying to utilize.  However, a common trick
in these types of methods to simply replace $x_{k+1}$ and $t_{k+1}$ in the
second and third arguments of $((x_{k+1}-x_k)/\Delta t, x_{k+1}, t_{k+1})$
with $x_k$ and $t_k$ in the Jacobian which gives the approximate Jacobian
%
\begin{equation}
M
= \frac{2}{\Delta t} \Jac{f}{\dot{x}}
+ \Jac{f}{x},
\label{rythmos:eqn:trap_M}
\end{equation}
%
where now all of these terms are evaluated at the point $(\dot{x},x,t) =
((x_{k+1}-x_k)/\Delta t, x_k, t_k)$.  It is easy to see that this is the same
form as the Jacobian in (\ref{rythmos:eqn:M}) where $\alpha = 2/\Delta t$ and
and $\beta = 1$.  The Jacobian in (\ref{rythmos:eqn:trap_M}) is not an exact
Jacobian for the equations in (\ref{rythmos:eqn:trap_dae_ne}) but can produce
good Newton steps in many cases.  However, an inexact Jacobian such as this
can cause a problem when sensitivity computations are being performed (see
Section {}\ref{rythmos:sec:adj-sens}).

\subsubsection{Generalized Alpha methods}

The Generalized-$\alpha$ methods can be roughly described as follows
{}\cite{ChungHulbert,JansenWhitingHulbert,BonelliBursiErlicherVulcan}.

We consider implicit ODEs where $\jac{f}{\dot{x}}$ is nonsingular but in the
future we may also consider the more general case where $\jac{f}{\dot{x}}$ is
rank deficient for the DAE case.  The main parameters of interest in this
class of methods,
\begin{tabbing}
\hspace{4ex}are\hspace{5ex}\= \\
\>	$\alpha_f$, which determines where $x$ is solved \\
\>	$\alpha_m$, which determines where $\dot{x}$ is solved \\
\>	$\gamma$, which determines the relationship between $x$ and $\dot{x}$. 
\end{tabbing}

If the method is to be second order, there is only once choice for $\gamma =
{}\frac{1}{2}+\alpha_m - {}\alpha_f$.  To maintain stability, $\alpha_f$ and
$\alpha_m$ must satisfy
%
\begin{equation}\label{rythmos:eqn:genalpha:stab}
\alpha_m \ge \alpha_t \ge \frac{1}{2}.
\end{equation}
%
It should be noted here, that $\alpha_f$ and $\alpha_m$ can be chosen such
that $t_{k+\alpha_f}$ and $t_{k+\alpha_m}$ lie outside the interval
$[t_k,t_{k+1}]$.

Given these three coefficients, $\Delta t^n$, and initial data for $x_k=(t_k)$
and $\dot{x}^n=\dot{x}(t_k)$, the procedure is as follows.  Form a predictor
for $x_{k+1}$:
%
\begin{equation} \label{rythmos:eqn:genalpha:predictor1}
x_{k+1}^{(0)} = x_k + \Delta t_k\dot{x}_k.
\end{equation}
%
Higher order predictors are computed from Adams-Bashforth predictors
\cite{AdamsBashforth}, in particular the second order predictor in this
context is:
%
\begin{equation} \label{rythmos:eqn:genalpha:predictor2}
x_{k+1}^{(0)} = x_k + \frac{\Delta t_k}{2}\left(2+\frac{\Delta t_k}{\Delta t_{k+1}}\right)\dot{x}_k
  - \frac{(\Delta t_k)^2}{2\Delta t^{n-1}}\dot{x}^{n-1}.
\end{equation}
%
Then we interpolate between $x_k$ and $x_{k+1}$ to obtain $x_{k+\alpha_f}$:
%
\begin{equation}\label{rythmos:eqn:genalpha:xalphaf}
x_{k+\alpha_f} = (1-\alpha_f)x_k + \alpha_f x_{k+1}^{(0)}.
\end{equation}
%
Now we compute an approximation to $\dot{x}_{k+\alpha_m}$:
%
\begin{equation}\label{rythmos:eqn:genalpha:xdotalpham}
\dot{x}_{k+\alpha_m}^{(0)} = \left(1-\frac{\alpha_m}{\gamma}\right)\dot{x}_k
+ \left(\frac{\alpha_m}{\alpha_f\Delta t_k\gamma}\right)
\left(x_{k+\alpha_f}^{(0)} - (1-\alpha_f)x_k\right).
\end{equation}
%
Given this approximation, we can solve the nonlinear problem:
%
\begin{equation}\label{rythmos:eqn:genalpha:nonlinear}
f(\dot{x}_{k+\alpha_m}^{(0)},x_{k+\alpha_f}^{(0)},t_{k+\alpha_f}) = 0.
\end{equation}
%
for $x_{k+\alpha_f}^{(0)}$ and then compute $\dot{x}_{k+\alpha_m}^{(0)}$ using
equation (\ref{rythmos:eqn:genalpha:xdotalpham}).  Finally, we interpolate
back to the desired output time:
%
\begin{eqnarray}\label{rythmos:eqn:genalpha:interp}
x_{k+1} & = & \left(1-\frac{1}{\alpha_f}\right)x_k + \frac{1}{\alpha_f}x_{k+\alpha_f} \\
\dot{x}^{n+1} & = & \left(1-\frac{1}{\alpha_m}\right)\dot{x}_k + \frac{1}{\alpha_m}\dot{x}_{k+\alpha_f}.
\end{eqnarray}

The theory for this family of methods comes from the existing methods that fall
out of this general formulation for specific choices of $\alpha_f$, $\alpha_m$,
and $\gamma$.  Also, no estimates of the local truncation error exist, so no
adaptive time stepping strategies are available.  

\subsubsection{Implicit Runge-Kutta methods}
\label{rythmos:sec:implicit-rk}

We now consider a class of powerful and popular one-step methods for solving
implicit DAEs, implicit Runge-Kutta (RK) methods.  The most general form
of implicit RK methods requires the simultaneous solution of $p$ sets
of coupled nonlinear equations that take the form
%
\begin{equation}
r_i(z) = f\left( \dot{x}_i, x_k + \Delta t \sum_{j=1}^{p} a_{ij} \dot{x}_j,
t_k + c_i \Delta t \right) = 0,
\; \mbox{for} \; i = 1 \ldots p
\label{rythmos:eqn:irk_dae_ne}
\end{equation}
%
where $\dot{x}_i$ are essentially approximations to the derivatives
$\dot{x}(t_k + c_i \Delta t)$ called {}\textit{stage derivatives} and $z = [
{}\dot{x}_1, {}\dot{x}_1, {}\ldots, {}\dot{x}_p ]^T$ are the unknowns in this
set of equations.  After this set of coupled equations is solved, the state
solution $x_{k+1}$ is given as the linear combination
%
\begin{equation}
x_{k+1} = x_k + \Delta t \sum_{i=1}^{p} b_i \dot{x}_j.
\end{equation}

Implicit RK methods are classified by both the order $p$ and the selection of
the constants $a_{ij}$, $c_i$ and $b_i$.  It is customary to consider these
constants in the form of the {}\textit{Butcher diagram}
%
\[
\begin{array}{c|c}
c & A \\
\hline
  & b^T
\end{array}
\; \; = \; \;
\begin{array}{c|cccc}
c_1 & a_{11} & a_{12} & \cdots & a_{1p} \\
c_2 & a_{21} & a_{22} & \cdots & a_{2p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
c_p & a_{p1} & a_{p2} & \cdots & a_{pp} \\
\hline
 & b_1 & b_2 & \cdots & b_p
\end{array}
\]
%
where $A = [a_{ij}]$, $b = [b_1, b_2, {}\ldots, b_p]^T$, and $c =
[c_1, c_2, {}\ldots, c_p]^T$.

It is clear how to form the residual for the fully coupled system for $r(z)$ in
(\ref{rythmos:eqn:irk_dae_ne}) just from individual evaluations $(\dot{x},x,t)
{}\rightarrow f$ but how the Newton system for such a system is solved will
vary greatly based on the structure and properties of the Butcher matrix $A$.

\subsubsection*{Fully implicit IRK methods}

Fully implicit IRK methods present somewhat of a problem for developing
general software since they involve the need to solve a fully coupled system
of $p$ sets of equations of the form (\ref{rythmos:eqn:irk_dae_ne}).  Each
block $\jac{r_i}{z_j} = {}\jac{r_i}{\dot{x}_j}$ of the full Jacobian
$\jac{r}{z}$ is represented as
%
\begin{equation}
\Jac{r_i}{\dot{x}_j}
= \Jac{f}{\dot{x}}
+ \Delta t a_{ij} \Jac{f}{x},
\; \mbox{for} \; i = 1 \ldots p, \; j = 1 \ldots p
\label{rythmos:eqn:dridzi_trap}
\end{equation}
%
which is evaluated at the points $(\dot{x},x,t) = ( {}\dot{x}_i, x_k +
{}\Delta t {}\sum_{j=1}^{p} a_{ij} {}\dot{x}_j, t_k + c_i {}\Delta t )$.  Note
that $\jac{r_i}{z_j} = {}\jac{r_i}{\dot{x}_j}$ in
(\ref{rythmos:eqn:dridzi_trap}) is of the same form as (\ref{rythmos:eqn:M})
where $\alpha = 1$ and $\beta = \Delta t a_{ij}$.

When considering a iterative method for solving systems with the block
operator matrix $\jac{r}{z}$, it is easy to see how to use the general matrix
$M$ in (\ref{rythmos:eqn:M}) to implement a matrix-vector product, but it is
not obvious how to precondition such a system.  Clearly a block diagonal
preconditioner could be used but the effectiveness of such a preconditioning
strategy is open to question.  Other preconditioning strategies are also
possible just given the basic block operators and this is an open area of
research.

In some cases, however, it may be possible to form a full matrix object for
$\jac{r}{z}$ but this is not something that can be expected for most
applications.

\subsubsection*{Semi-explicit IRK methods}

Semi-explicit IRK methods are those IRK methods where the Butcher matrix $A$
is lower diagonal and therefore gives rise to a block lower triangular
Jacobian matrix $\jac{r}{z}$.  For these types of methods, the nonlinear
equations in (\ref{rythmos:eqn:dridzi_trap}) can be solved one at a time for
$i = 1 {}\ldots p$ which is easily accommodated using a Newton-type method
where the Newton Jacobian for each $i$ is given by
(\ref{rythmos:eqn:dridzi_trap}), which is of our basic general form
(\ref{rythmos:eqn:M}).

\subsubsection*{Diagonally-implicit IRK methods}

The next specialized class of IRK methods that we consider are
diagonally-implicit IRK methods where the Butcher coefficients in $A$ and $c$
give rise to a lower triangular Jacobian $\jac{r}{z}$ (and hence are also
semi-explicit IRK methods) that has the same nonsingular matrix block of the
form in (\ref{rythmos:eqn:dridzi_trap}) along the diagonal.  This, of course,
requires that $a_{11} = a_{22} = {}\ldots = a_{pp}$ and $c_{1} = c_{2} =
{}\ldots = c_{p}$.  In this class of IRK methods, significant savings may be
achieved since a single set of linear solver objects (i.e.\ operators and
preconditioners) can be utilized for the solution of the fully coupled system.
In fact, it may even be possible to utilize multi-vector applications of the
operator and preconditioner for matrices of the form (\ref{rythmos:eqn:M})
which can be supported by many applications.

{}\textbf{ToDo: I need to write up some more ideas here to see how to take
advantage of multi-vector operations when possible and if it makes since to do
so.}

\subsubsection{Discontinuous Galerkin in time}

\cite{DGTime}
{}\textbf{Todd: Scott Collis mentioned this a something that can be important
from a variety of standpoints but I don't have any references for this yet.}

\subsubsection{Continuous Gelerkin in time }

\cite{CGTime}
{}\textbf{Todd: Scott Collis mentioned this a something that can be important
from a variety of standpoints but I don't have any references for this yet.}

%\subsection{General Solution Strategies for Implicit Nonlinear Time Step Equations}

\section{Staggered DAEs}
\label{rythmos:sec:staggered-daes}

A typical situation that occurs is when a set of two or more DAEs are solved
in which the solutions are propagated from one to the other.  Here we define
this type of DAE and show how these can be effectively dealt with.  In this
section we focus on just two staggered DAEs for simplicity but the same
principles apply to more than two staggered DAEs.

The model that we will use here is of two DAEs where the first DAE
%
\begin{eqnarray}
%
f_1(\dot{y}_1(t),x_1(t),t) & = & 0,
\; t \in \left[ t_0, t_f \right], \label{rythmos:eqn:stag:f1} \\
x_1(0) & = & x_{0,1}, \label{rythmos:eqn:stag:f1:ic} \\
%
\end{eqnarray}
%
is solved and who's solution $x_1(t)$is used to define a second set of DAEs of
the form
%
\begin{eqnarray}
%
f_2(\dot{x}_2(t),x_2(t),x_1(t),t) & = & 0,
\; t \in \left[ t_0, t_f \right], \label{rythmos:eqn:stag:f2} \\
x_2(0) & = & x_{0,2}, \label{rythmos:eqn:stag:f2:ic}
%
\end{eqnarray}
%
which are solved for $x_2(t)$, where $x_1\in\mathcal{X}_1$,
$\dot{x}_1\in\mathcal{X}_1$, $x_2\in\mathcal{X}_2$,
$\dot{x}_2\in\mathcal{X}_2$, $f_1(\dot{x}_1,x_1,t) {}\in {}\mathcal{X}_1^2
{}\times {}\RE {}\rightarrow {}\mathcal{F}_1$, $f_2(\dot{x}_2,x_2,t) {}\in
{}\mathcal{X}_2^2 {}\times {}\mathcal{X}_1 {}\times {}\RE {}\rightarrow
{}\mathcal{F}_1$, $x_{0,1}\in\mathcal{X}_1$, $\mathcal{X}_1 {}\in
{}\RE^{n_{x,1}}$, $\mathcal{X}_2 {}\in {}\RE^{n_{x,2}}$, $\mathcal{F}_1 {}\in
{}\RE^{n_{x,1}}$, and $\mathcal{F}_1 {}\in {}\RE^{n_{x,2}}$.

These types of two-level staggered DAEs come up over and over again in
different situations.  Staggered DAEs are used extensively in sensitivity
computations and show up in other contexts as well such as multi-physics
solutions.  Note that staggered DAEs, as shown above, are much simpler to
consider than fully coupled sets of DAEs (i.e.\ where each DAE is dependent on
the state of the other).  Issues related to fully coupled DAEs are a whole
other matter entirely and are beyond the scope of this discussion.

The staggered DAEs shown above can be solved in a number of ways.  The first
way to solve these staggered DAEs is to integrate them more-or-less together
but to solve for the time-step for
(\ref{rythmos:eqn:stag:f1})--(\ref{rythmos:eqn:stag:f1:ic}) first and then
solve for the time-step for
(\ref{rythmos:eqn:stag:f2})--(\ref{rythmos:eqn:stag:f2:ic}) second.  This
approach requires minimally that the solution for $x_1(t)$ be communicated in
some way to the second DAE
(\ref{rythmos:eqn:stag:f2})--(\ref{rythmos:eqn:stag:f2:ic}).  Just this
requirement in itself has a number of difficult issues associated with it.  We
discuss these and other issues in more detail latter, after we discuss the
second approach.

The second approach for solving the two staggered DAEs is to combine them into
one implicit DAE of the form in
(\ref{rythmos:eqn:dae})--(\ref{rythmos:eqn:dae:ic}) where
%
\begin{eqnarray*}
x & = & {\bmat{c} x_1 \\ x_2 \emat}, \\
f(\dot{x},x,t) & = & {\bmat{c} f_1(\dot{x}_1,x_1,t) \\  f_2(\dot{x}_2,x_2,x_1,t) \emat}, \\
x_0 & = & {\bmat{c} x_{0,1} \\ x_{0,2} \emat},
\end{eqnarray*}
%
and $\mathcal{X}=\mathcal{X}_1\times\mathcal{X}_2$ is a product vector space.
The combined DAE is then solved using a single time stepper algorithm.  Many
of the implicit time stepper methods described in
Section~\ref{rythmos:sec:implicit-time-steppers} would then need to solve the
implicit nonlinear system $r(z)=0$ that arises and a Newton-like method would
require solves with the composite matrix
%
\begin{equation}
\alpha \Jac{f}{\dot{x}} + \beta \Jac{f}{x}
=
{\bmat{cc}
\alpha \Jac{f_1}{\dot{x}_1} + \beta \Jac{f_1}{x_1}
& 0 \\
\beta \Jac{f_2}{x_1}
& \alpha \Jac{f_2}{\dot{x}_2} + \beta \Jac{f_2}{x_2} \\
\emat}.
\end{equation}
%
This block matrix is lower block triangular with nonsingular blocks along the
diagonal which are the customary matrices for each individual DAE.  The first
potential problem with the above block matrix is the lower left block $\beta
(\jac{f_2}{x_1})$ which is not required by any of the individual DAE solvers
(but is required by a direct sensitivity solver, see Section ???).  Therefore,
an exact Newton method requires a type of derivative computation that is not
required when the DAEs are solved individually.  However, in an iterative
method, only the action of $(\jac{f_2}{x_1}) V$ to arbitrary multivectors
$V\in\mathcal{X}_2|\RE^m$ are needed which can be approximated using
directional finite differences.

There are many different issues to consider when comparing and contrasting
these two basic approaches for solving staggered DAEs:
%
\begin{itemize}
%
{}\item {}\textbf{Communication of state information}: The one big advantage
of the combined DAE approach using a single time stepper algorithm is that the
time stepper itself can easily and transparently handle the communication of
$x_1$ from the first DAE to the second DAE.  For example, consider a
higher-level RK method where stage derivatives must be computed at Gauss
points.  In a single time stepper method this is a simple matter, but is more
difficult to handle when using two different time stepping algorithms (which
may have some data encapsulation).  This issue can be resolved if a general
interpolation mechanism where exposed by a time stepper where $x_1(t)$ at any
time in the time-step interval where easily accessible.
%
{}\item {}\textbf{Step-size and error control}: When the combined DAE is
integrated by a single time stepping method, step-size and error control is
automatically handled for both of the DAE solutions.  This may or may not be
desirable.  However, when the two DAEs are integrated separately using two
different time stepping methods, these two methods may not make the same
decisions as to the step sizes to take and this can cause a problem for a
variety of standpoints.  However, if the first time stepper where driven by
the interpolation interface for extracting $x_1(t)$ as called by the second
time integrator, then steps-size control would not be an issue.  In addition,
handling the errors separately may be an advantage since the these two DAEs
may have different solution error requirements.

Note: We need to look at the theory for the propagation of errors in a
staggered time stepper in order to be faithful about satisfying user-defined
error tolerances.  For example, if the first DAE is solved inexactly, then
this error will be propagated to the second DAE and will be compounded by the
error in solving the second DAE.  Therefore, one can not simply look at just
the errors in the two DAE separately if one wants to insure an accurate error
bound on the second DAE.
%
{}\item {}\textbf{Linearity, nonlinearity and relative cost}: One of the
biggest advantages of solving the staggered DAEs using separate time stepping
algorithms (or at least using a specialized nonlinear solver within a time
step) is that one of the DAEs may be linear while the other nonlinear and one
DAE may be much more expensive than the other to solve.  For example, if the
first DAE is nonlinear and small, but the second DAE is linear but much larger
and more expensive to solve than the first, then a combined Newton approach
will result in much more expense than if the two DAE time steps are computed
independently.  However, if the first DAE is inexpensive and well-conditioned
compared to the nonlinear second DAE, then there is little to no extra expense
in solving the DAEs as one large combined DAE.
%
{}\item {}\textbf{State derivatives}: As mentioned above, the combined DAE
approach using a single Newton method requires derivative computations
involving $(\jac{f_2}{x_1})$ which are not required when solving the DAEs
separately.  However, sensitivity methods do require access to this operator.
%
\end{itemize}

\section{Transient Sensitivity Computations}
\label{rythmos:sec:trans-sens}

In this section we describe the nuts and bolts associated with direct and
adjoint sensitivity methods that are derived in
Appendix~\ref{rythmos:app:sens-derivations}.

The general state DAE model for sensitivity computations that we
consider here takes the form
%
\begin{eqnarray}
c\left( \dot{y}(t), y(t), p, v(t), t \right) & = & 0,
\; t \in \left[ t_0, t_f \right], \label{rythmos:eqn:sens:c} \\
y(0) & = & y_0(p), \label{rythmos:eqn:sens:c:ic}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$y : t \rightarrow y(t) \:\in\:\mathcal{Y}$ are the transient state variables defined on $t\in[t_0,t_f]$, \\
\>	$\dot{y} : t \rightarrow \dot{y}(t) = d(y)/d(t)\:\in\:\mathcal{Y}$ are the temporal derivatives of $y$ defined on $t\in[t_0,t_f]$, \\
\>	$p \:\in\:\mathcal{P}$ are steady-state auxiliary parameters, \\
\>	$v : t \rightarrow v(t) \:\in\:\mathcal{V}$ are transient auxiliary parameters defined on $t\in[t_0,t_f]$, \\
\>	$c(\dot{y}, y, p, v, t) :
		\mathcal{Y}^2 \times \mathcal{P} \times \mathcal{V} \times \RE
		\rightarrow \mathcal{C}$ are the state DAE functions, \\
\>	$y_0(p) \:\in\:\mathcal{P} \rightarrow \mathcal{Y}$ define the initial condition for $y(t_0)$ as a function of $p$, \\
\>	$\mathcal{Y} \:\subseteq\:\RE^{n_y}$ is the vector space of the state variables $y(t)$, \\
\>	$\mathcal{P} \:\subseteq\:\RE^{n_p}$ is the vector space of the steady-state auxiliary variables $p$, \\
\>	$\mathcal{V} \:\subseteq\:\RE^{n_v}$ is the vector space of the transient auxiliary variables $v(t)$, and \\
\>	$\mathcal{C} \:\subseteq\:\RE^{n_y}$ is the vector space of the output of the DAE function $c(\ldots)$.
\end{tabbing}

The auxiliary parameters $p$ and $v$ in (\ref{rythmos:eqn:sens:c}) represent
adjustable variables that can be manipulated for some purpose and, once
selected, determine a fully specified set of DAEs that can be solved forward
in time for the state variables $y$.  The form of the DAE in
(\ref{rythmos:eqn:sens:c}), which has auxiliary variables, arises in a number
of contexts other than the sensitivity computations being considered here.

We now define a composite set of auxiliary response functions for which
sensitivities will be computed
%
\begin{equation}
d(y,p,v)
= \int_{t_0}^{t_f} g(y(t),p,v(t),t) dt + h(y(t_f),p),
\label{rythmos:eqn:sens:d}
\end{equation}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$g(y,p,v,t) : \:
		\mathcal{Y} \times \mathcal{P} \times \mathcal{V} \times \RE
		\rightarrow \mathcal{G}$ are distributed response functions defined on $t\in[t_0,t_f]$, \\
\>	$h(y,p) : \: \mathcal{Y} \times \mathcal{P}
		\rightarrow \mathcal{G}$ are terminal response functions defined only at $t=t_f$, and \\
\>	$\mathcal{G} \:\subseteq\:\RE^{n_g}$ is the vector space of the auxiliary response functions.
\end{tabbing}

{}\textit{Remark on notation:} In this discussion we use the unadorned
identifiers $y$ and $v$ to represent the entire infinite-dimensional functions
over the domain $t\in[t_0,t_f]$ and in some contexts also the values of these
functions at points $t$.  In some cases, $y(t)\in\mathcal{Y}$ and
$v(t)\in\mathcal{V}$ are used to denote specific values of these functions to
remove ambiguity.  In general, the meaning of $y$ and $v$ as either the entire
functions or as particular values of these functions at points $t$ should be
clear from the context in which these identifiers are used.  In cases where
confusion may result, the exact meaning will be explained.

Note that the domain $(y,p,v)$ for the overall response function $d(y,p,v)$ is
infinite since $y$ and $v$ are infinite in the continuous formulation.


The implicit state solution for the DAEs in
(\ref{rythmos:apdx:eqn:sens:c})--(\ref{rythmos:apdx:eqn:sens:c:ic}) at points
$t$ is signified as $y(p,v,t)\in\mathcal{Y}$, where $v$ in this case is the
selection of transient parameters in the range $t\in[t_0,t]$.  Note that
selections for $v$ in the range $(t:t_f]$ have no influence on the implicit
state solution $y$ up to the time $t$.  The full infinite-dimensional implicit
state solution for $y$ in $t\in[t_0,t_f]$ is signified as $y(p,v)$ where in
this case $v$ represents the entire selection for the transient parameters in
$[t_0,t_f]$.

The implicit state solution $y(p,v,t)$ substituted into
(\ref{rythmos:eqn:sens:d}) gives the reduced auxiliary response functions
%
\begin{equation}
\hat{d}(p,v)
= \int_{t_0}^{t_f} \hat{g}(p,v(t),t) dt + \hat{h}(p),
\label{rythmos:eqn:sens:d_hat}
\end{equation}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$\hat{g}(p,v(t),t) = g(y(p,v,t),p,v(t),t) : \:
		\mathcal{P} \times \mathcal{V} \times \RE	\rightarrow \mathcal{G}$, and \\
\>	$\hat{h}(p) = h(y(p,v,t),p) : \: \mathcal{P}	\rightarrow \mathcal{G}$.
\end{tabbing}

Below, we will consider direct and adjoint approaches to the computation of
the first derivative object
%
\begin{equation}
\Jac{\hat{d}}{p} \in \mathcal{G}|\mathcal{P}
\label{rythmos:eqn:d_d_hat_d_p}
\end{equation}
%
and adjoint approaches for the computation of
%
\begin{equation}
\Jac{\hat{d}}{v(t)} \in \mathcal{G}|\mathcal{V}, \; t \in [t_i,t_f].
\label{rythmos:eqn:d_d_hat_d_v}
\end{equation}
%
Note that it is generally not computationally tractable to compute a discrete
approximation to the first derivative object (\ref{rythmos:eqn:d_d_hat_d_v})
involving the transient auxiliary variables $v$ because of the large
discretized dimension of $v$.

\subsection{Transient direct sensitivity computations}
\label{rythmos:sec:direct-sens}

The direct sensitivity method for computing the reduced first derivative
object $\jac{\hat{d}}{p}$, as derived in
Appendix~\ref{rythmos:app:direct-sens-derivation}, involves first solving the
$n_p$ independent direct sensitivity equations
%
\begin{eqnarray}
%
\Jac{c}{\dot{y}} \dot{S} + \Jac{c}{y} S + \Jac{c}{p}
& = & 0, \; t \in \left[ t_0, t_f \right], \label{rythmos:eqn:sens:direct-c} \\
S(t_0) & = & \Jac{y_0}{p}, \label{rythmos:eqn:sens:direct-c:ic}
\end{eqnarray}
%
followed by the computation of the reduced derivatives
%
\begin{equation}
\Jac{\hat{d}}{p} 
= \int_{t_0}^{t_f} \left( \Jac{g}{y} S +  \Jac{g}{p} \right) dt
+ \left. \left(  \Jac{h}{y} S + \Jac{h}{p} \right) \right|_{t=t_f},
\label{rythmos:eqn:sens:d_d_hat_d_p_2}
\end{equation}
%
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\> $S = \jac{y}{p} {}\in {}\mathcal{Y}|\mathcal{P}$ is the
direct sensitivity of the state at $t\in[t_0,t_f]$, \\
\> $\dot{S} = \frac{d}{dt}\left(\jac{y}{p}\right)
{}\in {}\mathcal{Y}|\mathcal{P}$ is the time derivative of $S$ at $t\in[t_0,t_f]$, \\
\> $\jac{c}{{}\dot{y}} {}\in {}\mathcal{C}|\mathcal{Y}$ is defined on
$t\in[t_0,t_f]$, \\
\> $\jac{c}{y} {}\in {}\mathcal{C}|\mathcal{Y}$ is defined on $t\in[t_0,t_f]$, \\
\> $\jac{c}{p} {}\in {}\mathcal{C}|\mathcal{P}$ is defined on $t\in[t_0,t_f]$, \\
\> $\jac{y_0}{p} {}\in {}\mathcal{Y}|\mathcal{P}$ is defined only at $t=t_0$, \\
\> $\jac{{}\hat{g}}{p} {}\in {}\mathcal{G}|\mathcal{P}$ is defined on $t\in[t_0,t_f]$, \\
\> $\jac{{}\hat{h}}{p} {}\in {}\mathcal{G}|\mathcal{P}$ is defined only at $t=t_f$, and \\
\> $\jac{{}\hat{d}}{p} {}\in {}\mathcal{G}|\mathcal{P}$ is defined independent of time.
\end{tabbing}

This computation is performed by integrating the state
(\ref{rythmos:eqn:sens:c})--(\ref{rythmos:eqn:sens:c:ic}) together with the
direct sensitivity equations
(\ref{rythmos:eqn:sens:direct-c})--(\ref{rythmos:eqn:sens:direct-c:ic}) and
the integral in (\ref{rythmos:eqn:sens:d_d_hat_d_p_2}) is computed while the
DAEs are being integrated.

There are a variety of approaches for integrating the state and direct
sensitivity equations (see {}\cite{cvodes}) but arguably the the more
efficient approach is to solve for the (nonlinear) time step for the state
equations (\ref{rythmos:eqn:sens:c}) first, followed by computing the linear
time step for the direct sensitivities (\ref{rythmos:eqn:sens:direct-c}), and
then finally evaluating the integral in
(\ref{rythmos:eqn:sens:d_d_hat_d_p_2}).

A few important properties of the direct sensitivity equations are worth
noting: they are linear, and the sensitivity DAE for each sensitivity
parameter $p_i$ ($i=1\ldots{}n_p$) are independent.

Note that the evaluation of the residual for the direct sensitivity equations
in (\ref{rythmos:eqn:sens:direct-c}) requires the computation of the composite
operator multivector product
%
\[
\Jac{c}{\dot{y}} \dot{S} + \Jac{c}{y} S
\]
%
where $\dot{S}, S {}\in\mathcal{Y}|\mathcal{G}$ are any arbitary multivectors.
This is functionality that is not directly used by a forward time integrator
but the general form of $M$ in (\ref{rythmos:eqn:M}) supports this by
computing two operators with $(\alpha=1,\beta=0)$ and $(\alpha=0,\beta=1)$.

Some tips from DASPK~3.0~\cite{new-daspk} for solving this problem are:
\begin{itemize}
%
{}\item First, solve state equation, applying error control, to convergence for a
time step for the state.
%
{}\item Next, solve sensitivity equations using a Newton method (even though the problem
is linear) as this gives a more stable solution.  The RHS for the Newton method yields
a better solution that the RHS for the direct linear solve.
%
{}\item Consider truncation error for each set of parameters independently and take
the max for the overall error control and step-size selection.
%
\end{itemize}

Some tips from CVODES~\cite{cvodes} for solving this problem are:
\begin{itemize}
%
{}\item The linear sensitivity systems are also solved using a Newton method,
mainly to allow the use of out-of-date Jacobians.
%
{}\item Step-size control may or may not consider the sensitivity parameters.
%
{}\item Error control of the integration of the quadrature variables does
not seem to be taken into account.
%
\end{itemize}

Note that for the purposes of a general time stepper algorithm that accepts
the general DAE formulation $f(\dot{x},x,t)$, it is easy to cast the direct
sensitivity equations in that form by first defining the composite vector
%
\begin{equation}
x =
{\bmat{c}
S_{(:,1)} \\ \vdots \\ S_{(:,n_p)}
\emat}
\in \mathcal{Y}^{n_p}
\end{equation}
%
consisting of the columns $S_{(:,i)}$ of $S$ and then defining a similar
composite DAE function
%
\begin{equation}
f(\dot{x},x,t) =
{\bmat{c}
%
\Jac{c}{\dot{y}} \dot{S}_{(:,1)}
+ \Jac{c}{y} S_{(:,1)}
+ \Jac{c}{p_{(1)}} \\
%
\vdots \\
%
\Jac{c}{\dot{y}} \dot{S}_{(:,n_p)}
+ \Jac{c}{y} S_{(:,n_p)}
+ \Jac{c}{p_{(n_p)}}
%
\emat}
\in \mathcal{Y}^{n_p} \rightarrow \mathcal{C}^{n_p}.
\label{rythmos:eqn:direct-sens-daes}
\end{equation}
%
The matrix $M$ in (\ref{rythmos:eqn:M}) for
(\ref{rythmos:eqn:direct-sens-daes}) is given by
%
\begin{equation}
\alpha \Jac{f}{\dot{x}} + \beta \Jac{f}{x} = 
{\bmat{cccc}
\alpha \Jac{c}{\dot{y}} + \beta \Jac{c}{y} \\
& \alpha \Jac{c}{\dot{y}} + \beta \Jac{c}{y} \\
& & \ddots \\
& & & \alpha \Jac{c}{\dot{y}} + \beta \Jac{c}{y}
\emat}
\in \mathcal{C}^{n_p}|\mathcal{Y}^{n_p}
\label{rythmos:eqn:direct-sens-daes-jac}
\end{equation}
%
which is a block diagonal matrix with the same block along the diagonal and
all quantities are evaluated at the same trial point $(\dot{y},y,p,t)$ as
defined by the time integration algorithm.  Note that in the software
implementation using Thyra, it is easy to cast back and forth between a
multi-vector and vector view of the same data and it is therefore easy to take
advantage of multi-vector computations even through the time integrator has no
idea that multi-vectors are being used under the covers.

With the addition of quadrature variables $w(t)\in\mathcal{G}$ and the
associated quadrature ODEs $\dot{w}(t) = g(y(t),p,v(t),t)$, the sensitivity
problem can be transformed to remove an explicit integration of the the
distributed response function $g(y(t),p,v(t),t)$ from the problem as seen by
the DAE integrator.  This approach is used in CVODES and DASPK 3.0 to some
extent and this has advantages and disadvantages.  This creates essentially
another combined set of staggered DAEs that are solved by a single time
stepper algorithm.  This may imply that the error in the quadrature equations
are considered when taking a time step for the direct sensitivity equations
which may or may not be desirable.  The quadrature equations can also be
treated as an extra set of staggered DAEs which again has advantages and
disadvantages.

Note that the structure and properties of the direct sensitivity equations in
(\ref{rythmos:eqn:sens:direct-c}) can exploited in a few different ways
as described in the following subsections.

\subsubsection{Block linear solvers}

Different blocks of sensitivities (i.e.\ for $i = i_k {}\ldots i_{k+1}$) can
be solved at one time using block direct or iterative linear solvers.  The use
of block solvers in CVODES, IDAS or DASPK 3.0 is not possible given the
structure of these codes.  The use of block iterative linear solver (such as
block GMRES in Belos [???])  may speed up the calculation by an order of
magnitude or more compared to solving the linear systems one RHS at a time.

Because of the use of block linear solvers, our direct sensitivity DAE solvers
may achieved as much as an order of magnitude of speedup or more over a solver
such as CVODES that only allows for solving sensitivity systems one RHS at a
time (see the function {}\texttt{cv\_lsolveS(...)} in
{}\cite{cvodes-users-guide}).

\subsubsection{Exploitation of parallelism for blocks for sensitivities}

Since each direct sensitivity equation is independent of the others, they can
be solved on parallel blocks and parallel clusters of processors.  For
example, consider a DAE that comes from a discretized PDE who's state is
distributed over 10 processors and has $n_p=100$ parameters.  The solution of
the direct sensitivity equations can be solved on a single chuck of 10
processors where all 100 DAEs are solved simultaneously using a block solver.
However, the direct sensitivity DAEs can also be solved on 10 chucks of 10
processors each (i.e.\ 100 overall processors) where blocks of 10 direct
sensitivity DAEs are solved on each chunk of 10 processors.  However, when
using this approach, note that in general the state DAE has to be integrated
on each of the chunks of processors along with the direct sensitivity DAEs and
this will severely impact parallel speedup efficiency.  However, if the state
DAE is linear then the state is not needed to define the direct sensitivity
DAEs and therefore no state computation is needed in order to solve the direct
sensitivity DAEs.  In this case, better parallel speedup can be achieve to
larger numbers of chunks of processors.  However, parallel speedup will be
limited when the numbers of sensitivity DAEs becomes too small on each chunk
of processors since the block linear solver will slow down (because of slower
flop rates due to reductions in cache efficiency and increased relative global
communication overhead).

\subsection{Transient adjoint sensitivity computations}
\label{rythmos:sec:adj-sens}

The most general statement of the adjoint equations associated with the state
equations in (\ref{rythmos:eqn:sens:c})--(\ref{rythmos:eqn:sens:c:ic}) and the
response functions in (\ref{rythmos:eqn:sens:d}), as derived in
Appendix~\ref{rythmos:app:adj-equ-derivation}, is
%
\begin{eqnarray}
- \frac{d}{dt}\left( \Jac{c}{\dot{y}}^T \Lambda \right)
+  \Jac{c}{y}^T \Lambda + \Jac{g}{y}^T
& = & 0, \; t \in \left[ t_0, t_f \right],
\label{rythmos:app:eqn:sens:adj} \\
\left.\left( \Jac{c}{\dot{y}}^T \Lambda \right)\right|_{t=t_f}
& = & - \left. \Jac{h}{y}^T \right|_{y=y(t_f)},
\label{rythmos:app:eqn:sens:adj:fc}
\end{eqnarray}
%
where $\Lambda\in\mathcal{C}|\mathcal{G}$ is the multi-vector of Lagrange
multipliers at $t\in[t_0,t_f]$.  Here we consider index-0 and index-1 DAEs.
Note that for index-0 DAEs (which includes implicit and explicit ODEs) that
the mass matrix $\jac{c}{\dot{y}}$ is non-singular and therefore the final
condition (\ref{rythmos:app:eqn:sens:adj:fc}) becomes trivially easy to solve
as it just requires an inversion of $\jac{c}{\dot{y}}$ (assuming that the mass
matrix is easy to invert).  For index-1 DAEs, the computation of the final
condition (\ref{rythmos:app:eqn:sens:adj:fc}) is more involved.

As the adjoint is being computed, the integral
%
\begin{equation}
\Jac{\hat{d}}{p}^T =
\int_{t_0}^{t_f} \left(
    \Jac{g}{p}^T
    + \Jac{c}{p}^T \Lambda
  \right) dt
  + \left. \Jac{h}{p}^T \right|_{t=t_f}
  - \left. \left( \Jac{y_0}{p}^T \Jac{c}{\dot{y}}^T \Lambda \right) \right|_{t=t_0}
\label{rythmos:eqn:sens:d_d_hat_d_p}
\end{equation}
%
and the point-wise gradient
%
%
\begin{equation}
\Jac{\hat{d}}{v(t)}^T
= \Jac{g}{v(t)}^T + \Jac{c}{v(t)}^T \Lambda(t),
t\in[t_0,t_f]
\label{rythmos:eqn:sens:d_d_hat_d_v_t}
\end{equation}
%
can both be computed (backwards in time).  Note that these computations depend
on the state $y(t)$ as well as on the adjoint $\Lambda(t)$ in general and
therefore the state and the (local) adjoint history are both needed.  If the
integral equation is computed using quadrature variables, then getting access
to $y(t)$ and $\Lambda(t)$ is automatic but if these equations are integrated
using a different time stepping algorithm (working backward in time), then
more general interpolation access to $y(t)$ and $\Lambda(t)$ will be required.

The approaches used to solve the adjoint equations differ depending on whether
$\jac{c}{\dot{y}}$ is constant or non-constant and both cases are discussed in
the next two subsections.

\subsubsection{Non-augmented adjoint equations for constant mass matrices}

When the mass matrix $\jac{c}{\dot{y}}$ is constant, then
(\ref{rythmos:app:eqn:sens:adj}) reduces to
%
\begin{eqnarray}
- \Jac{c}{\dot{y}}^T \dot{\Lambda}
+  \Jac{c}{y}^T \Lambda + \Jac{g}{y}^T
& = & 0, \; t \in \left[ t_0, t_f \right],
\label{rythmos:app:eqn:sens:adj-const-mass} \\
\left.\left( \Jac{c}{\dot{y}}^T \Lambda \right)\right|_{t=t_f}
& = & - \left. \Jac{h}{y}^T \right|_{y=y(t_f)},
\label{rythmos:app:eqn:sens:adj-const-mass:fc}
\end{eqnarray}
%
where $\dot{\Lambda}=d(\Lambda)/d(t)\in\mathcal{C}|\mathcal{G}$.

The adjoint DAE can be cast in the standard form $f(\dot{x},x,t)$ understood
by a general time integrator by first defining the composite vector
%
\begin{equation}
x =
{\bmat{c}
\Lambda_{(:,1)} \\ \vdots \\ \Lambda_{(:,n_g)}
\emat}
\in \mathcal{C}^{n_g}
\end{equation}
%
consisting of the columns $\Lambda_{(:,j)}$ of $\Lambda$ and then defining the
associated composite DAE function
%
\begin{equation}
f(\dot{x},x,t) =
{\bmat{c}
%
- \Jac{c}{\dot{y}}^T \dot{\Lambda}_{(:,1)}
+  \Jac{c}{y}^T \Lambda_{(:,1)} + \Jac{g_{(1)}}{y}^T
%
\\ \vdots \\
%
- \Jac{c}{\dot{y}}^T \dot{\Lambda}_{(:,n_g)}
+  \Jac{c}{y}^T \Lambda_{(:,n_g)} + \Jac{g_{(n_g)}}{y}^T
%
\emat}
\in \mathcal{C}^{n_p} \rightarrow \mathcal{Y}^{n_p}.
\label{rythmos:eqn:adj-sens-daes}
\end{equation}
%
Note also that a transformation in time for the sensitivity equations of $\tau
= t_f - t$ is needed to allow the integration backward in time using a forward
time integrator.

The matrix $M$ in (\ref{rythmos:eqn:M}) for
(\ref{rythmos:eqn:adj-sens-daes}) is given by
%
\begin{equation}
\alpha \Jac{f}{\dot{x}} + \beta \Jac{f}{x}
= 
{\bmat{ccc}
\alpha \Jac{c}{\dot{y}}^T + \beta \Jac{c}{y}^T \\
& \ddots \\
& & \alpha \Jac{c}{\dot{y}}^T + \beta \Jac{c}{y}^T
\emat}
\in \mathcal{Y}^{n_p}|\mathcal{C}^{n_p}
\label{rythmos:eqn:adj-sens-daes-jac}
\end{equation}
%
which is a block diagonal matrix with the same block along the diagonal and
all quantities are evaluated at the same trial point $(\dot{y},y,p,v,t)$ as
defined by the time integration algorithm.  An important property of this
block matrix is that each block along the diagonal is the adjoint of the
matrix ${}\alpha (\jac{c}{\dot{y}}) + {}\beta (\jac{c}{y})$ which is needed by
the implicit forward and direct direct sensitivity solvers.  In general, the
adjoint time stepper can be implemented using the same sensitivity objects as
required by forward and direct sensitivity solves with the added requirement
that adjoint operations with these object also need to be supported (see
Section~\ref{rythmos:sec:general-dae-requirements}).

Also note that the block diagonal form of
(\ref{rythmos:eqn:adj-sens-daes-jac}) immediately lends itself for use with a
block linear solver.  When $n_g$ is larger, then a block linear solver may
provide significant speedups over a single RHS linear solver implementation
such as is the case in CVODES and IDAS (see ??? in [???] for example).

\subsubsection{Augmented adjoint equations for non-constant mass matrices}

When the mass matrix $\jac{c}{\dot{y}}$ is a function of $t$ and/or $y$ then
the adjoint equations in
(\ref{rythmos:app:eqn:sens:adj})--(\ref{rythmos:app:eqn:sens:adj:fc}) are
transformed into the {}\textit{augmented adjoint equations} which take the
form
%
\begin{eqnarray}
\frac{d}{dt}\left( \bar{\Lambda} \right)
+  \Jac{c}{y}^T \Lambda + \Jac{g}{y}^T
& = & 0, \; t \in \left[ t_0, t_f \right],
\label{rythmos:app:eqn:sens:aug-adj-de} \\
\bar{\Lambda} + \Jac{c}{\dot{y}}^T \Lambda
& = & 0, \; t \in \left[ t_0, t_f \right],
\label{rythmos:app:eqn:sens:aug-adj-ae} \\
\left. \bar{\Lambda} \right|_{t=t_f}
& = & \left. \Jac{h}{y}^T \right|_{y=y(t_f)}.
\label{rythmos:eqn:sens:aug-adj:fc}
\end{eqnarray}

The augmented adjoint system can be stated in the standard form understood by
general time integrators by first defining the composite vector
%
\begin{equation}
x =
{\bmat{c}
{\bmat{c} \bar{\Lambda}_{(:,1)} \\ \Lambda_{(:,1)} \emat} \\
\vdots \\
{\bmat{c} \bar{\Lambda}_{(:,n_g)} \\ \Lambda_{(:,n_g)} \emat} \\
\emat}
\in \mathcal{C}^{2 n_g}
\end{equation}
%
consisting of the columns $\bar{\Lambda}_{(:,j)}\in\mathcal{C}$ and
$\Lambda_{(:,j)}\in\mathcal{C}$ of $\bar{\Lambda}$ and $\Lambda$,
respectively, and then defining the associated composite DAE function
%
\begin{equation}
f(\dot{x},x,t) =
{\bmat{c}
%
{\bmat{c}
\frac{d}{dt}\left( \bar{\Lambda}_{(:,1)} \right)
+ \Jac{c}{y}^T \Lambda_{(:,1)} + \Jac{g_{(1)}}{y}^T
\\
\bar{\Lambda}_{(:,1)} + \Jac{c}{\dot{y}}^T \Lambda_{(:,1)}
\emat} \\
%
\vdots \\
%
{\bmat{c}
\frac{d}{dt}\left( \bar{\Lambda}_{(:,n_g)} \right)
+ \Jac{c}{y}^T \Lambda_{(:,n_g)} + \Jac{g_{(n_g)}}{y}^T
\\
\bar{\Lambda}_{(:,n_g)} + \Jac{c}{\dot{y}}^T \Lambda_{(:,n_g)}
\emat} \\
%
\emat}
\in \mathcal{C}^{2 n_p} \rightarrow \mathcal{Y}^{2 n_p}.
\label{rythmos:eqn:aug-adj-sens-daes}
\end{equation}
%
Note also that a transformation in time for the sensitivity equations of $\tau
= t_f - t$ is needed to allow the integration backward in time using a forward
time integrator.

The matrix $M$ in (\ref{rythmos:eqn:M}) for
(\ref{rythmos:eqn:aug-adj-sens-daes}) is given by
%
\begin{equation}
\alpha \Jac{f}{\dot{x}} + \beta \Jac{f}{x}
= 
{\bmat{ccc}
%
{\bmat{cc}
\alpha I & \beta \Jac{c}{y}^T \\
\beta I &  \beta \Jac{c}{\dot{y}}^T
\emat} \\
%
& \ddots \\
%
& & {\bmat{cc}
\alpha I & \beta \Jac{c}{y}^T \\
\beta I &  \beta \Jac{c}{\dot{y}}^T
\emat}
%
\emat}
\in \mathcal{Y}^{2 n_p}|\mathcal{C}^{2 n_p}
\label{rythmos:eqn:aug-adj-sens-daes-jac}
\end{equation}
%
which is a block diagonal matrix with the same block along the diagonal and
all quantities are evaluated at the same trial point $(\dot{y},y,p,v,t)$ as
defined by the time integration algorithm.  At first glance, the blocks along
the diagonal
%
\begin{equation}
{\bmat{cc}
\alpha I & \beta \Jac{c}{y}^T \\
\beta I &  \beta \Jac{c}{\dot{y}}^T
\emat} \in \mathcal{Y}^{2}|\mathcal{C}^{2}
\label{rythmos:eqn:aug-adj-sens-jac}
\end{equation}
%
do not seem to correspond to the adjoint of the general form shown for the
matrix $M$ in (\ref{rythmos:eqn:M}) that we are trying to exploit.  However,
to show how to utilize the general form of $M$ in (\ref{rythmos:eqn:M}),
consider the solution of any arbitrary system of the form
%
\begin{equation}
{\bmat{cc}
\alpha I & \beta \Jac{c}{y}^T \\
\beta I &  \beta \Jac{c}{\dot{y}}^T
\emat}
{\bmat{c} v_1 \\ v_2 \emat}
=
{\bmat{c} b_1 \\ b_2 \emat}
\label{rythmos:eqn:aug-adj-sens-system}
\end{equation}
%
for arbitrary vectors $v_1, v_2 {}\in\mathcal{C}$ and $b_1, b_2
{}\in\mathcal{Y}$.  Apply the transformation
%
\[
{\bmat{cc}
-\frac{\beta}{\alpha} I & I \\
 & I
\emat}
\left[
{\bmat{cc}
\alpha I & \beta \Jac{c}{y}^T \\
\beta I &  \beta \Jac{c}{\dot{y}}^T
\emat}
{\bmat{c} v_1 \\ v_2 \emat}
=
{\bmat{c} b_1 \\ b_2 \emat}
\right]
\]
%
yields the transformed system
%
\begin{equation}
{\bmat{cc}
 & \bar{\alpha} \Jac{c}{\dot{y}}^T + \bar{\beta} \Jac{c}{y}^T \\
\beta I &  \beta \Jac{c}{\dot{y}}^T
\emat}
{\bmat{c} v_1 \\ v_2 \emat}
=
{\bmat{c}  b_2 - \bar{\gamma} b_1 \\ b_2 \emat}
\label{rythmos:eqn:aug-adj-sens-trans-system}
\end{equation}
%
where $\bar{\alpha}=\beta$, $\bar{\beta}=\beta^2/\alpha$, and
$\bar{\gamma}=\beta/\alpha$.  This system can then be solved as
%
\[
v_2 = \left( \bar{\alpha} \Jac{c}{\dot{y}}^T + \bar{\beta} \Jac{c}{y}^T \right)^{-1}
\left(  b_2 - \bar{\gamma} b_1 \right)
\]
%
followed by
%
\[
v_1 = \frac{1}{\beta} \left( b_2 - \beta \Jac{c}{\dot{y}}^T v_2 \right).
\]
%
The matrix
%
\[
\bar{\alpha} \Jac{c}{\dot{y}}^T + \bar{\beta} \Jac{c}{y}^T
\]
%
is known as the Schur complement and in this case is of the same general form
needed for implicit forward and direct sensitivity solvers and no new
functionality over what is needed for the solution of the non-augmented
adjoint DAE form is required for the solution of the augmented adjoint system.
Solving the augmented adjoint DAE system only requires a few extra operations
but requires twice the time-step storage since both $\bar{\Lambda}$ and
$\Lambda$ need to be maintained.

Again, these computations can be rearranged so that multi-vector operations
and block solvers can be utilized resulting in significant speedups over a
single vector RHS version such as in CVODES and IDAS.


\section{Stability Analysis and Eigen Problems}

{}\textbf{ToDo: Ask Andy what these methods are ask him to help fill in
requirements.}

\section{General DAE Formulation and Requirements}
\label{rythmos:sec:general-dae-requirements}

In this section we state the requirements for a general DAE to support various
time integration methods, direct and adjoint sensitivity computations, and
other types of numerical problems.  The most fundamental implementation of the
interface described here will be provided by the underlying application.  Our
goal here is to model the requirements at a level of abstraction most
convenient for application developers.  However, many other composite DAE
objects will also exist that will be based on the basic DAE abstractions.

Here we define the concept of a parameterized general DAE (or ODE) that takes
the form
%
\begin{eqnarray}
c\left( \dot{y}(t), y(t), \{u_l\}, t \right) & = & 0,
\; t \in \left[ t_0, t_f \right] \label{rythmos:eqn:gen:c} \\
y(t_0) & = & y_0(\{u_l\}) \label{rythmos:eqn:gen:c:ic}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$y(t) \:\in\:\mathcal{Y}$ are the transient state variables, \\
\>	$\dot{y}(t) = d(y)/d(t)\:\in\:\mathcal{Y}$ is the vector of temporal derivatives of $y(t)$, \\
\>	$\{u_l\}$ is a set of auxiliary parameter vectors with $|\{u_l\}| = N_u$ members, \\
\>	$u_l\in\mathcal{U}_l$, $l=1 {}\ldots N_u$, is the $l^{\mbox{th}}$ subvector of auxiliary parameters, \\
\>	$c(\dot{y}(t), y(t), \{u_l\}, t) :
		\mathcal{Y}^2 \times \mathcal{U}_1 \times \ldots \times \mathcal{U}_{N_u} \times \RE
		\rightarrow \mathcal{C}$ are the state DAEs, \\
\>	$y_0(\{u_l\}) \:\in\:\mathcal{U}_1 \times \ldots \times \mathcal{U}_{N_u} \rightarrow \mathcal{Y}$
		define the initial conditions for $y(t_0)$ as a function of $\{u_l\}$, \\
\>	$\mathcal{Y} \:\subseteq\:\RE^{n_y}$ is the vector space of the state variables $y(t)$, \\
\>	$\mathcal{U}_l \:\subseteq\:\RE^{n_{u,l}}$ is the vector space of $u_l$ ($l = 1 {}\ldots N_u$), \\
\>	$\mathcal{C} \:\subseteq\:\RE^{n_y}$ is the vector space of the output of the DAE function $c(\ldots)$.
\end{tabbing}

The DAE formulation above in
(\ref{rythmos:eqn:gen:c})--(\ref{rythmos:eqn:gen:c:ic}) differs slightly than
the sensitivity DAE formulation in
(\ref{rythmos:eqn:sens:c})--(\ref{rythmos:eqn:sens:c:ic}) in that the latter
is more specific to sensitivity computations while the former is more general
and more amiable to a general software implementation.  The DAE in
(\ref{rythmos:eqn:gen:c})--(\ref{rythmos:eqn:gen:c:ic}) is meant to represent
the most general DAE formulation that allows the composition of multiple DAEs
(for example in a staggered mode).  Different subvectors of auxiliary
parameters $u_l\in\mathcal{U}_l$ can be mapped into different types of
quantities in different types of numerical methods.  Some of the kinds of
roles that an individual subset vector $u_l\in\mathcal{U}_l$ of auxiliary
parameters can represent are:
%
\begin{itemize}
%
{}\item Steady-state parameters for sensitivity computations (i.e.\ $u_1 =
p$ in (\ref{rythmos:eqn:sens:c}))
%
{}\item Transient parameters for sensitivity computations (i.e.\ $u_2 = v(t)$
in (\ref{rythmos:eqn:sens:c}))
%
{}\item Coupling variables between different DAEs (e.g.\ $u_1 = x_1$ in
(\ref{rythmos:eqn:stag:f2}))
%
{}\item Continuation parameters (i.e.\ to aid in the solution of the implicit
nonlinear time step equations $r(z)=0$ in (\ref{rythmos:eqn:r}) or in the
solution of initial conditions)
%
{}\item Uncertain parameters (i.e.\ to be manipulated in an uncertainty
quantification study)
%
\end{itemize}

From the DAE developer's point of view, how a particular set of parameters is
used in some higher-level numerical algorithm does not greatly influence what
is required from the DAE to allow the manipulation of these parameters or in
how derivatives for those variables are computed.  The general DAE model in
(\ref{rythmos:eqn:gen:c})--(\ref{rythmos:eqn:gen:c:ic}) can be used, for
example, to represent
(\ref{rythmos:eqn:sens:c})--(\ref{rythmos:eqn:sens:c:ic}) in a sensitivity
computation, or can represent each DAE in a staggered set of DAEs, or any
other type of DAE.

For the purpose of sensitivity computations we will also define a general
set of auxiliary response functions of the form
%
\begin{equation}
g_j(y,\{\hat{u}_l\},t) \in \mathcal{U}_1 \times \ldots \times \mathcal{U}_{\hat{N}_u} \rightarrow \mathcal{G}_j, j=1 \ldots N_g, 
\label{rythmos:eqn:gen:g}
\end{equation}
%
The purpose of defining a general set of response functions in
(\ref{rythmos:eqn:gen:g}) is to provide a concise description of the
requirements for various types of computations.  The reason for defining
different sets of response functions $g_j(\ldots)$ is that any particular
member may represent a high-dimensional function with a specialized scalar
product for its vector space $\mathcal{G}_j$.  Note that computing
sensitivities for a a very high dimensional vector function $g_j(\ldots)$ is
intractable for an adjoint approach but is very a direct sensitivity approach
(note: the steady-state interface for nonlinear problems should be changed in
this way to support large dimensional response functions as well).

Note that the evaluations for $c(\dot{y},y,\{u_l\},t)$ and $g_j(y,\{u_l\}t)$
(for $j = 1 {}\ldots N_g$) may share common subcomputations and therefore
adding provisions to allow efficient simultaneous computations of all of the
objects is advisable.  In addition, the derivative computations described
below will automatically compute the function values when AD is used so a
means to allow the simultaneous computation of derivative objects along with
function values is needed for optimal performance.  Also, if the DAE
implementation does copy the state then it would be advisable to tell the DAE
when the state being passed in has changed or not.

Here we divide up the requirements for general DAE in
(\ref{rythmos:eqn:gen:c})--(\ref{rythmos:eqn:gen:c:ic}) and the response
functions in (\ref{rythmos:eqn:gen:g}) into three different categories:
forward solve, direct sensitivities and adjoint sensitivities.

\subsection{Requirements for forward solve}

Here we describe requirements for
(\ref{rythmos:eqn:gen:c})--(\ref{rythmos:eqn:gen:c:ic}) needed to
implement a basic class of implicit forward integration methods.

\begin{itemize}

\item\textbf{Zero-order requirements (explicit and implicit time steppers)}

\begin{itemize}
%
{}\item Initial condition evaluation: Given $u_l\in\mathcal{U}_l$ (for
$l=1\ldots{}N_u$) evaluate the initial condition function
\[
\{u_l\} {}\rightarrow y_0(\{u_l\})\in\mathcal{Y}.
\]
%
{}\item DAE residual evaluation: Given selections for $\dot{y}\in\mathcal{Y}$,
$y\in\mathcal{Y}$, $u_l\in\mathcal{U}_l$ (for $l=1\ldots{}N_u$), and
$t\in\RE$, evaluate
\[
(\dot{y},y,\{u_l\},t) {}\rightarrow c(\dot{y},y,\{u_l\},t)\in\mathcal{C}.
\]
%
{}\item Auxiliary response evaluation: Given selections for $y\in\mathcal{Y}$,
$u_l\in\mathcal{U}_l$ (for $l=1\ldots{}N_u$), and $t\in\RE$ evaluate
\[
\{u_l\} {}\rightarrow g(\{u_l\},t)\in\mathcal{G}_j, j = 1 \ldots N_g.
\]
%
\end{itemize}

\item\textbf{First-order requirements (implicit time steppers and sensitivity computations)}

\begin{itemize}
%
{}\item Formation of $M = {}\alpha (\jac{c}{\dot{y}}) + {}\beta (\jac{c}{y})$:
Given selections for $\dot{y}\in\mathcal{Y}$, $y\in\mathcal{Y}$,
$u_l\in\mathcal{U}_l$ (for $l=1\ldots{}N_u$), $t\in\RE$, $\alpha\in\RE$, and
$\beta\in\RE$ evaluate an abstract (nonsingular) object
\[
(\dot{y},y,\{u_l\},t) {}\rightarrow M
= {}\alpha \Jac{c}{{}\dot{y}} + \beta \Jac{c}{y}
{}\in {}\mathcal{C}|\mathcal{Y}.
\]
Note the special cases of $(\alpha,\beta)=(1,0)$ and $(\alpha,\beta)=(0,1)$
allow a client to access the operators $(\jac{c}{\dot{y}})$ and $(\jac{c}{y})$
separately.  Of course the operator returned for $M = \jac{c}{\dot{y}}$ for
the case of $(\alpha,\beta)=(1,0)$ is only guaranteed to be nonsingular for an
ODE.  For general DAEs, only operators $M$ for reasonable finite nonzero
values of $\alpha$ and $\beta$ are guaranteed to be nonsingular.
%
{}\item Allow reuse of factorizations, preconditioners, and other such objects
form some previously created operator $M_{\mbox{old}}$ in the reformation of
$M = {}\alpha (\jac{c}{\dot{y}}) + {}\beta (\jac{c}{y})$: Given a previously
computed nonsingular operator $M_{\mbox{old}}$, create a new operator $M$ at a
new point $(\dot{y},y,\{u_l\},t)$ that reuses any factorizations or
preconditioners from $M_{\mbox{old}}$ but represents the correct analytic
operator.  This capability is needed to allow a time stepper algorithm to have
some control over when preconditioners or factorizations are updated.
However, the newly computed operator $M$ must still provide a sufficient
analytic approximation to the true matrix which is needed for sensitivity
computations.
%
{}\item Application of $M = {}\alpha (\jac{c}{\dot{y}}) + {}\beta
(\jac{c}{y})$: Given a pre-formed object for $M = {}\alpha (\jac{c}{\dot{y}})
+ {}\beta (\jac{c}{y})$, provide the ability to apply the operator as
\[
Y = M X
\]
where $X\in\mathcal{Y}|\RE^m$ is some arbitrary RHS $m$-column multi-vector
and $Y\in\mathcal{C}|\RE^m$ is the LHS multi-vector.  The purpose of stating
the multi-vector form is to encourage the development of block methods that
are useful for other types of more advanced algorithms.  However, a single
vector version is sufficient to satisfy this requirement.
%
{}\item Solve with $M = {}\alpha (\jac{c}{\dot{y}}) + {}\beta (\jac{c}{y})$:
Given a pre-formed object for $M = {}\alpha (\jac{c}{\dot{y}}) + {}\beta
(\jac{c}{y})$, then provide the ability to (approximately) solve for systems
of the form
\[
X = M^{-1} B
\]
where $B\in\mathcal{Y}|\RE^m$ is some arbitrary RHS $m$-column multi-vector
and $X\in\mathcal{C}|\RE^m$ is the LHS solution multi-vector.  The purpose of
stating the linear solve requirement in multi-vector form is to encourage the
development of block linear solver approaches that are useful for other types
of more advanced algorithms.  However, note that the ability to solve single
linear systems of the form
\[
x = M^{-1} b,
\]
where $b\in\mathcal{Y}$ and $x\in\mathcal{C}$, automatically satisfies this
requirement.  The exact specification of what is meant to (approximately)
solve systems of this nature must be accurately specified. Note that a status
test that will work for composite and block linear solvers is needed which is
different than for a straightforward iterative or direct linear solver.
%
{}\item{} [Optional] Formation and maintenance of multiple $M = {}\alpha
(\jac{c}{\dot{y}}) + {}\beta (\jac{c}{y})$ objects: Some types of numerical
algorithms can be implemented much more efficiently if more than one instance
of $M = {}\alpha (\jac{c}{\dot{y}}) + {}\beta (\jac{c}{y})$ can be computed
and maintained simultaneously.  This may be needed, for instance, in an
implicit RK method as described in Section~\ref{rythmos:sec:implicit-rk}.
%
\end{itemize}

\end{itemize}

\subsection{Requirements for first-order direct sensitivities}

In addition for the requirements for forward solves described in the previous
section, here we describe an additional set of requirements for
(\ref{rythmos:eqn:gen:c})--(\ref{rythmos:eqn:gen:c:ic}) needed to implement
the implicit direct sensitivity methods as described in
Section~\ref{rythmos:sec:direct-sens}.

\begin{itemize}

{}\item Initial condition direct sensitivity: Given $u_l\in\mathcal{U}_l$ (for
$l=1\ldots{}N_u$) and some $U_l\in\mathcal{U}_l|\RE^m$ compute the action of
\[
(\{u_l\},U_l) {}\rightarrow \Jac{y_0}{u_l} U_l \in \mathcal{Y}|\RE^m,
\; \mbox{for} \; l = 1 {}\ldots N_u,
\]
or alternatively evaluate the sensitivities themselves as a multi-vector
\[
\{u_l\} {}\rightarrow \Jac{y_0}{u_l} \in \mathcal{Y}|\mathcal{U}_l,
\; \mbox{for} \; l = 1 {}\ldots N_u.
\]

{}\item DAE auxiliary direct sensitivities: Given selections for
$\dot{y}\in\mathcal{Y}$, $y\in\mathcal{Y}$, $u_l\in\mathcal{U}_l$ (for
$l=1\ldots{}N_u$), $t\in\RE$, and some $U_l\in\mathcal{U}_l|\RE^m$ compute
the action of
\[
(\dot{y},y,\{u_l\},t,U_l) {}\rightarrow \Jac{c}{u_l} U_l \in \mathcal{C}|\RE^m,
\; \mbox{for} \; l = 1 {}\ldots N_u,
\]
or alternatively compute the sensitivities themselves as a multi-vector
\[
(\dot{y},y,\{u_l\},t) {}\rightarrow \Jac{c}{u_l} \in \mathcal{C}|\mathcal{U}_l,
\; \mbox{for} \; l = 1 {}\ldots N_u.
\]

{}\item Auxiliary response function state direct sensitivities: Given
selections for $y\in\mathcal{Y}$, $u_l\in\mathcal{U}_l$ (for
$l=1\ldots{}N_u$), $t\in\RE$ , and some $V\in\mathcal{Y}|\RE^m$
compute the action of
\[
(y,\{u_l\},V) {}\rightarrow \Jac{g_j}{y} V \in \mathcal{G}|\RE^m,
\; \mbox{for} \; j = 1 \ldots N_g
\]
or alternatively compute the sensitivities themselves as a multi-vector
\[
(y,\{u_l\},V) {}\rightarrow \Jac{g_j}{y} \in \mathcal{G}|\mathcal{Y},
\; \mbox{for} \; j = 1 \ldots N_g.
\]

{}\item Auxiliary response function auxiliary parameter direct
sensitivities: Given selections for $y\in\mathcal{Y}$,
$u_l\in\mathcal{U}_l$ (for $l=1\ldots{}N_u$), $t\in\RE$ , and some
$U_l\in\mathcal{U}_l|\RE^m$ compute the action of
\[
(y,\{u_l\},U_l) {}\rightarrow \Jac{g_j}{u_l} U_l \in \mathcal{G}|\RE^m,
\; \mbox{for} \; l = 1 {}\ldots N_u, j = 1 \ldots N_g
\]
or alternatively compute the sensitivities themselves as a multi-vector
\[
(y,\{u_l\},U_l) {}\rightarrow \Jac{g_j}{u_l} \in \mathcal{G}|\mathcal{U}_l,
\; \mbox{for} \; l = 1 {}\ldots N_u, j = 1 \ldots N_g.
\]

\end{itemize}

Note that these three different types of forward derivative computations can
always be approximated using directional finite differences of $\{u_l\}
{}\rightarrow y_0$, $(\dot{y},y,\{u_l\},t) {}\rightarrow c$ and
$(\{\hat{u}_l\},t) {}\rightarrow g$, respectively.  Therefore, numerical
methods that only require forward sensitivities as described above can always
use directional finite differences and therefore require really no extra
functionality beyond what is needed for the forward solve.  Some numerical
methods, such as sensitivity computations, do require some degree of exactness
from derivative computations used for $M = {}\alpha (\jac{c}{\dot{y}}) +
{}\beta (\jac{c}{y})$ and the other derivative objects described above.

\subsection{Requirements for first-order adjoint sensitivities}

In this section we describe an extra set of requirements for implementing
adjoint methods.  Basically, these requirements state that the derivative
objects described for the direct sensitivity methods need to also support
adjoints.

\begin{itemize}

{}\item Initial condition adjoint sensitivity: Given $u_l\in\mathcal{U}_l$ (for
$l=1\ldots{}N_u$) and some $V\in\mathcal{C}|\RE^m$ compute the action of
\[
(\{u_l\},U_l) {}\rightarrow \Jac{y_0}{u_l}^T V \in \mathcal{U}_l|\RE^m
\; \mbox{for} \; l = 1 {}\ldots N_u,
\]
or alternatively evaluate the sensitivity themselves as a multi-vector
\[
\{u_l\} {}\rightarrow \Jac{y_0}{u_l} \in \mathcal{Y}|\mathcal{U}_l
\; \mbox{for} \; l = 1 {}\ldots N_u.
\]
%
{}\item Application of adjoint operator $M^T = ({}\alpha (\jac{c}{\dot{y}}) +
{}\beta (\jac{c}{y}))^T$: Given a pre-formed object for $M = {}\alpha
(\jac{c}{\dot{y}}) + {}\beta (\jac{c}{y})$, provide the ability to apply the
operator as
\[
Y = M^T X
\]
where $X\in\mathcal{C}|\RE^m$ is some arbitrary RHS $m$-column multi-vector
and $Y\in\mathcal{Y}|\RE^m$ is the LHS multi-vector.  The purpose of stating
the multi-vector form is to encourage the development of block methods that
are useful for other types of more advanced algorithms.  However, the ability
to apply the operator to single multi-vectors automatically satisfies this
requirement.
%
{}\item Solve with adjoint operator $M^T = ({}\alpha (\jac{c}{\dot{y}}) +
{}\beta (\jac{c}{y}))^T$: Given a pre-formed object for $M = {}\alpha
(\jac{c}{\dot{y}}) + {}\beta (\jac{c}{y})$, provide the ability to
(approximately) solve for systems of the form
\[
X = M^{-H} B
\]
where $B\in\mathcal{C}|\RE^m$ is some arbitrary RHS $m$-column multi-vector
and $X\in\mathcal{Y}|\RE^m$ is the LHS solution multi-vector.  The purpose of
stating the linear solve requirement in multi-vector form is to encourage the
development of block linear solver approaches that are useful for other types
of more advanced algorithms.  However, note that the ability to solve single
linear systems of the form
\[
x = M^{-H} b,
\]
where $b\in\mathcal{C}$ and $x\in\mathcal{Y}$, automatically satisfies this
requirement.  The exact specification of what is meant to (approximately)
solve systems of this nature must be accurately specified. Note that a status
test that will work for composite and block linear solvers is needed which is
different than for a straightforward iterative or direct linear solver.
%
{}\item DAE auxiliary adjoint sensitivities: Given selections for
$\dot{y}\in\mathcal{Y}$, $y\in\mathcal{Y}$, $u_l\in\mathcal{U}_l$ (for
$l=1\ldots{}N_u$), $t\in\RE$, and some $V\in\mathcal{C}|\RE^m$ compute
the action of
\[
(\dot{y},y,\{u_l\},t,V) {}\rightarrow \Jac{c}{u_l}^T V \in \mathcal{U}_l|\RE^m,
\; \mbox{for} \; l = 1 {}\ldots N_u,
\]
or alternatively compute the sensitivities themselves as a multi-vector
\[
(\dot{y},y,\{u_l\},t) {}\rightarrow \Jac{c}{u_l} \in \mathcal{C}|\mathcal{U}_l,
\; \mbox{for} \; l = 1 {}\ldots N_u.
\]

{}\item Auxiliary response function state adjoint sensitivities: Given selections for
$y\in\mathcal{Y}$, $u_l\in\mathcal{U}_l$ (for $l=1\ldots{}N_u$), $t\in\RE$ ,
and some $V\in\mathcal{G}_j|\RE^m$ compute the action of
\[
(y,\{u_l\},V) {}\rightarrow \Jac{g_j}{y}^T V \in \mathcal{Y}|\RE^m,
\; \mbox{for} \; j = 1 \ldots N_g
\]
or alternatively compute the sensitivities themselves as a multi-vector
\[
(y,\{u_l\},V) {}\rightarrow \Jac{g_j}{y} \in \mathcal{G}|\mathcal{Y},
\; \mbox{for} \; j = 1 \ldots N_g.
\]

{}\item Auxiliary response function auxiliary parameter adjoint
sensitivities: Given selections for $y\in\mathcal{Y}$,
$u_l\in\mathcal{U}_l$ (for $l=1\ldots{}N_u$), $t\in\RE$ , and some
$V\in\mathcal{G}_j|\RE^m$ compute the action of
\[
(y,\{u_l\},V) {}\rightarrow \Jac{g_j}{u_l}^T V \in \mathcal{U}_l|\RE^m,
\; \mbox{for} \; l = 1 {}\ldots N_u, j = 1 \ldots N_g
\]
or alternatively compute the sensitivities themselves as a multi-vector
\[
(y,\{u_l\},V) {}\rightarrow \Jac{g_j}{u_l} \in \mathcal{G}|\mathcal{U}_l,
\; \mbox{for} \; l = 1 {}\ldots N_u, j = 1 \ldots N_g.
\]

\end{itemize}

Note that unlike the direct sensitivity requirements, these adjoints can not
be simulated using directional finite differences.  In general, the best way
to perform these adjoint computations is a tailored use of the reverse mode of
AD.  Or, if the matrix and multi-vector objects can be created explicitly,
then they can just be transposed (or conjugate transpose in the complex case)
to provide the needed adjoint operations.

\section{General Adaptive Error-Control Strategies}

RAB's comments on this:
\begin{itemize}
%
{}\item It seems that error control, step size control and order control is
very integral in a specific class of DAE and ODE solvers.  Therefore, as a
first pass, it may just make sense to just make an atomic time stepping
algorithm define the time step equations and perform step-size and order
control.
%
{}\item After some more thought, there is little reason to allow mixing and
matching of time stepping methods for predictors and correctors.  While the
predictor is essentially an explicit time stepping method there are other ways
of reusing common code than to reuse entire time stepping objects for
predictors.
%
{}\item External clients should be able to replace the error and step-size
control algorithms in addition to being able to adjust the ``magic''
parameters.
%
{}\item For staggered DAEs, an overall error control strategy should be
composable just from local truncation error estimates from atomic time
stepping methods.
%
\end{itemize}

\section{Checkpointing and Interpolation Storage Management Strategies}

Checkpointing a transient iterative computation (or any other expensive
iterative computation) is an important capability in large-scale scientific
computing.  A checkpoint is a snap shoot of the state of an iterative
computation that can be saved to a hard disk and then read back in later to
restart the computation from the checkpoint from such that it results in the
same computations as if the computations where never interrupted.  There are
various levels of checkpoints but in its most basic form we use the term
checkpoint to mean that the state is written in at a level such then when read
back in to a time integrator that is results in the exact same
{}\underline{binary} computations that would have resulted if the computation
was not restarted.

Checkpoints are primarily needed for the following use cases:
%
\begin{itemize}
%
{}\item {}\textbf{Fault tolerance}: By periodically checkpointing an iterative
solve on an MPP, if one or more processors goes down, the computation can be
restarted on a different set of processors from the last checkpoint.  Note
that in this case, one can only expect the same binary output from the
simulation if using the same number and same type of processors and if the
results of MPI global reductions is the same for the new processor topology.
Also note that this type of checkpointing requires that the checkpoints be
saved in such a way that they can be retrieved if one or more processors goes
down.  This means that using local disk storage on a processor is less
attractive than using a globally shared disk array.
%
{}\item {}\textbf{Recomputation of the state for adjoint computations}:
Another big motivator for checkpointing capability is the need to recompute
the state equation (\ref{rythmos:eqn:sens:c})--(\ref{rythmos:eqn:sens:c:ic})
when the adjoint sensitivity equation
(\ref{rythmos:app:eqn:sens:adj})--(\ref{rythmos:app:eqn:sens:adj:fc}) is
solved backwards in time.  This topic is discussed in more detail below.
These types of checkpoints may best be stored on a local disk on each
processor in an MPP which results in faster I/O.  Note that this type of
checkpointing and the more traditional fault-tolerant checkpointing, where
checkpoints are written to a nonlocal disk array, may both be appropriate in
an adjoint computation.
%
\end{itemize}

In general, each time stepper algorithm will be responsible for specifying how
to checkpoint it's algorithm with the least possible data such that a restart
will result in the same binary results.

Related to checkpointing in the need to interpolate an integrated solution.
Interpolation is needed for the following use cases:
%
\begin{itemize}
%
{}\item {}\textbf{Allowing clients to pick out solutions at specific time
points}: There are situations where clients need to extract the state solution
at specific time points in order to, for instance, compare to some other
estimate of the state such as in a least-squares problem.  Another example of
this is in extracting the state at some final time $t_f$ which the time
integrator may not necessary stop exactly on.  For example, if the Jacobian is
being reused for the last time step it may be more efficient to actually
integrate past $t_f$ and then let the user pick out $t_f$ using some
interpolation.
%
{}\item {}\textbf{Interpolation of the state for the backward adjoint
computations}: In general, the backward integration of the adjoint equation
(\ref{rythmos:app:eqn:sens:adj})--(\ref{rythmos:app:eqn:sens:adj:fc}) will not
fall directly on the same time steps used to compute the state equation
(\ref{rythmos:eqn:sens:c})--(\ref{rythmos:eqn:sens:c:ic}), especially if
adaptive time-stepping is used.  Therefore, there must be some general means
for a time integrator to record a history of its solution in an interpolation
buffer that can then be queried for values of the sate $y(t)$ and perhaps even
the time derivative $\dot{y}(t)$ at specific times $t$.  Interpolation will be
discussed in more detail later.
%
{}\item {}\textbf{Interpolation of DAE solutions in decoupled staggered DAE
time steppers}: On way to solve staggered DAEs, such as described in
Section~\ref{rythmos:sec:staggered-daes}, is to use separate time integrators
for the first and second DAEs.  This allows different time stepping algorithms
tailored to each set of DAEs and result in an overall better time integration
scheme.  This however, requires that the second DAE be able to read the state
from the first DAE in a general way.  If the first time stepper algorithm can
provide an interpolation buffer that can be queried for $x_1(t)$ and/or
$\dot{x}_1(t)$ by the second DAE, then the two time stepper algorithms can be
completely decoupled.  Note that the interpolation buffer needed in this use
case is likely much shorter than would be needed for an interpolation of the
state solution as required by an adjoint time integrator.
%
\end{itemize}

A few ideas regarding checkpointing and interpolation are:
\begin{itemize}
%
{}\item A time stepper (or a series of staggered time steppers) restarted form
a checkpoint should produce the exact same (binary) output as if the
integration never stopped.  This makes a checkpoint a special time event that
affects a lot of other things.
%
{}\item A time stepper should be able to write out $(y(t),\dot{y}(t))$ at the
end if each time step so that a Hermite interpolant can be achieved.  Or, an
abstract interpolation object (which may be specific to a time integrator)
should be constructed and maintained by a time integrator.
%
\end{itemize}

\subsection{Interpolation methods}

While there are many possible interpolation techniques for interpolating the
solution of a DAE, the Hermite interpolation seems to be the most
popular~\cite{sundials}.  The simplest Hermite interpolation stores both
$y(t_k)$ and $\dot{y}(t_k)$ at various points in time $t_k$ and then the
values of $y(t)$ (and $\dot{y}(t)$) are interpolated using a piecewise cubic
polynomial formed from $y(t_k)$, $\dot{y}(t_k)$, $y(t_{k+1})$, and
$\dot{y}(t_{k+1})$, where $t\in[t_k,t_{k+1}]$.  The Hermite interpolate is a
local computation and is globally both zero-order and first-order continuous.

Note that interpolation methods may also take into account the accuracy needed
in the adjoint so that a more course interpolation buffer can be stored (Note:
ask Andrei about this).

\subsection{Checkpointing for adjoint sensitivity computations}

The adjoint solution of
(\ref{rythmos:app:eqn:sens:adj})--(\ref{rythmos:app:eqn:sens:adj:fc}) requires
access to the state $y(t)$ as it is integrated backwards in time.  If there is
sufficient storage, then a complete interpolation buffer for $y(t)$ for
$t\in[t_0,t_f]$ can be stored while the state equation
(\ref{rythmos:eqn:sens:c})--(\ref{rythmos:eqn:sens:c:ic}) is integrated
forward in time.  This state interpolation buffer can then be used to
integrate the adjoint backward in time.  However, if there is not sufficient
disk space to store a sufficiently accurate interpolation buffer, then this
approach will not work and a checkpoint strategy must be employed.

Checkpointing strategies\footnote{Note that the term ``checkpoint'' is being
somewhat overloaded here in the context of transient adjoint problems and is
more entailed than the basic definition of a checkpoint of a single state and
previously defined.} for adjoint problems store a series of snapshots and then
recompute the state again one or more times in smaller time segments in order
to build smaller temporary interpolation buffers that can then be used to
integrate the adjoint backward in time one piece at a time.

The question to ask is, ``Given a limited interpolation buffer (to be stored
in core) and a maximum number of checkpoints that can be stored (as allowed by
the disk storage), how should checkpoints be set that result in the minimum
amount of recomputation of the forward state equation?''  This question is
answered the checkpointed algorithm of Griwank~\cite{347846} and is
encapsulated in a simple program called {}\texttt{revolve()}~\cite{347846}.
The algorithm implemented in this simple driver program directs the forward
checkpointing and reverse adjoint computations in such a way as to provably
minimize the amount of needed recomputation of the forward problem.  The key
to using {}\texttt{revolve()} is to realize that {}\texttt{revolve}'s concept
of a ``step'' is the length of the forward computation that can be stored in
an incore interpolation buffer or as a set of computations such that the
operation count swaps individual checkpoint management.
Figure~\ref{rythmos:fig:checkpoint_graph} shows a plot of the relative
increase in cost in recomputing the forward solution for various ratios of the
number of checkpoint stored with respect to the total number of ``steps'' (as
defined by {}\texttt{resolve}).  Amazingly, as little as a 1\% ratio of
checkpoints to ``steps'' only increases the number of forward computation by
2.7!  It is not until the ratio of checkpoints to ``time'' steps goes below
0.1\% does the added cost starts to dramatically increase and asymptotically
approaches infinity as the ratio goes to zero.

{\bsinglespace
\begin{figure}
\begin{center}
%\fbox{
\includegraphics*[scale=0.75]{checkpoint_graph}
%}
\end{center}
\caption{
\label{rythmos:fig:checkpoint_graph}
Plot of increased cost of computing adjoint using the checkpoint placement
scheme implemented by the {}\texttt{revolve()} algorithm relative to cost of a
single forward solve for where the ratio of the number of checkpoints to the
total time steps is the x-axis where 5000 total time steps where used.  }
\end{figure}
\esinglespace}

Note: The {}\texttt{revolve()} function has several basic problems -- such as
the use of static data, global namespace pollution, explicit calls to
{}\texttt{printf()}, the use of ``magic numbers'', and non-intuitive calling
sequences -- that makes it inappropriate for large-scale production programs.
However, this code can easily be refactored into a more general C++ class that
allows for multiple instances and better usability.  The question is how to
build general software around a simple utility such as this.

\subsection{Checkpoint compression strategies}

If one is willing to give up binary restart-ability for the state computation
and is willing to allow some greater degree error in the adjoint sensitivity
computation, then compression strategies for the state can be used to even
further reduce memory storage.  For all DAEs, compression in time is possible
where interpolates are spaced out according to some error bound.  For DAEs
that come from discretized PDEs, compression in space and even the
computation of a courser state equation is possible if only inexact
adjoints are needed.

%
% Note: This may be a good angle for a research proposal.  How do we combine
% knowledge about inexactness with an adjoint time integrator and allow for
% compressed interpolation and checkpointing strategies and control of adaptive
% step-control error?  This leads into an inexactness proposal.
%

\section{Example Applications}

Notes: The purpose of this section is to describe some important Sandia
applications and describe how the propose time integration tools may be used
with these applications.

\subsection{Xyce}

Xyce \cite{Xyce,Xyce_time} is an analog circuit simulator capable of efficiently
simulating circuits on large scale distributed memory computers.  Circuits are
passed to Xyce through netlists which are text files containing lists of
devices and their network connectivity.  These devices are typically described
by DAEs, but there are a few devices which are described best by
integro-differential equations (in particular, the transmission line device).
These latter devices are handled in an approximate way while maintaining a
circuit DAE of the form:

\begin{eqnarray}\label{rythmos:eqn:xyce:dae}
\dot{q}(x)+f(x)+b(t) & = & 0 \\
x(0) & = & x_0.
\end{eqnarray}

In this formulation, $q$ represents the charge and flux contributions from
linear and nonlinear capacitors and inductors, $f$ represents the linear and
nonlinear current contributions from devices like the diode and the BJT, and
$b$ represents the source terms to the circuit.   SPICE \cite{Spice} solves
this DAE by using first and second order implicit BDF methods for ODEs and
asking the time integrator to differentiate $q$ when forming the residual.
Xyce has traditionally followed this approach, but an effort is underway which
solves the DAE equation (\ref{rythmos:eqn:xyce:dae}) directly using the
implicit BDF methods from IDA \cite{IDA}.   The circuits of interest to Xyce so
far have not had a DAE index greater than 1, and often the DAE is simply an ODE
with linear algebraic constraints.  Due to the desire to solve large scale
problems, parallelization has been incorporated which requires the use of
preconditioners for Jacobian solves with the implicit BDF methods.  In general,
preconditioners must be tuned for each circuit due to the inherently widely
varying behavior of circuit equations.

\subsection{Premo}

\subsection{Charon}

\subsection{Sundance}

\section{Conclusions}

% ---------------------------------------------------------------------- %
% References
%
\clearpage
\bibliographystyle{plain}
\bibliography{RythmosDesignSAND}
\addcontentsline{toc}{section}{References}

% ---------------------------------------------------------------------- %
% Appendices should be stand-alone for SAND reports. If there is only
% one appendix, put \setcounter{secnumdepth}{0} after \appendix
%
\appendix

\section{Notation}

Here the basic notational convetions that are used throught this
document are defined and explained.

\subsection{Basic Vector and Linear Operator Notation}

Here we define the basic notational constructs that are used in this document.
First, all vectors, multi-vectors and linear operators have associated vector
spaces that must be considered.  Suppose we have the vector spaces
$\mathcal{X}$ and $\mathcal{Y}$.  The vectors $x\in\mathcal{X}$ and
$y\in\mathcal{Y}$ are said to be members of these vector spaces.  A linear
operator $A$ that maps vectors from the {}\textit{domain} vector space
$\mathcal{X}$ to the {}\textit{range} vector space $\mathcal{Y}$ is denoted
$A\in\mathcal{Y}|\mathcal{X}$.  In some contexts, vectors themselves can be
used as linear operators.  For example, the vector $x\in\mathcal{X}$ can be
thought of as the linear operator $x\in\mathcal{X}|\RE$ and it's adjoint can
be applied to another vector $v\in\mathcal{X}$ as $x^T v {}\in\RE$ which is
exaclty equivalent to the definition of the space $\mathcal{X}$'s scalar
product
%
\[
x^T v \equiv \left< x, v \right>_{\mathcal{X}}.
\]
%
The convension of interpreting $x^T$ as a linear operator that performs the
operation of scalar product greatly simplifies many different formulations.

Note that in general that the adjoint operator $A^T\in\mathcal{Y}|\mathcal{X}$
must agree with its non-transposed operator $A\in\mathcal{Y}|\mathcal{X}$ by
the relation
%
\[
v^T ( A u ) = \left< v, A u \right>_{\mathcal{F}} = \left< A^T v, u \right>_{\mathcal{X}} = u^T (A^T v)
\]
%
where $u\in\mathcal{X}$, $v\in\mathcal{Y}$, and $<.,.>_{\mathcal{X}}$ and
$<.,.>_{\mathcal{Y}}$ are the scalar products associated with the vector
spaces $\mathcal{X}$ and $\mathcal{Y}$ respectively.

A multivector $V\in\mathcal{X}|\RE^{m}$ is a special kind of linear operator
in that its domain space is simply the $m$-dimensional Euclidean space and its
columns are generally considered to be dense vectors.  In some contexts,
multivectors appear as arguments of a linear operator
$A\in\mathcal{Y}|\mathcal{X}$ such as
%
\[
Y = A X
\]
%
where $X\in\mathcal{X}|\RE^m$ and $Y\in\mathcal{Y}|\RE^m$.  This dual nature
of a multivector (i.e.\ both as a set of concatonated vectors for arguments to
other linear operators and as linear operators themselves) greatly simplifies
the notation of various formulations.

\subsection{Partial Derivative Operator Notation}

The purpose of this section is to carefully define various partial derivative
operators that arise in the context of sensitivity an other computations.
These objects primarily will be defined based on what they can do when
combined with other vector and multi-vector objects.  Note that a
multi-vector, in addition to being input and output data from these linear
operations, is itself a linear operator and therefore all of the same
principles apply.

In this section we will deal with various partial derivative operator objects
for the vector function
%
\begin{equation}
(x,y) \rightarrow f(x,y) \in \mathcal{X} \times \mathcal{Y} \rightarrow \mathcal{F}
\end{equation}
%
where $x\in\mathcal{X}$, $y\in\mathcal{Y}$, $\mathcal{X}\subseteq\RE^{n_x}$,
$\mathcal{Y}\subseteq\RE^{n_y}$, and $\mathcal{F}\subseteq\RE^{n_f}$.  Some of
the second derivative computations will only be defined for the case of
$n_f=1$ (i.e.\ $\mathcal{F}=\RE$).

{}\textit{Remarks on notation:} The $i^{\mbox{th}}$ component of a vector $x$
is denoted $x_{(i)}$ and the $j^{\mbox{th}}$ component vector function of
$f(x,y)$ is denoted $f_{(j)}(x,y)$ or simply $f_{(j)}$.

For the purpose of defining various multi-vector operations we define another
vector space $\mathcal{S}\subset\RE^{n_s}$.

Each of these linear operations are defined in component-wise form consistent
with Euclidean vector spaces.  The extention to arbitary vector spaces (with
arbitrary scalar products) should be strightforward.

\subsubsection{First-Order (Jacobian) Operators}

We represent the first derivative operator of $f(x,y)$ evaluated at some point
$(x,y)$ as
%
\begin{equation}
\Jac{f}{x}\in\mathcal{F}|\mathcal{X},
\end{equation}
%
which defines a linear operator.  An object of this type if often refered to
as a matrix and in componentwise form is
%
\begin{equation}
\left(\Jac{f}{x}\right)_{(i,j)} = \Jac{f_{(i)}}{x_{(j)}} \in\RE, \; i = 1 \ldots n_f, \; j = 1 \ldots n_x.
\end{equation}

Note that here $\partial x$ can represent $\partial x$ or $\partial y$ in
which case $n_x$ and $\mathcal{X}$ is replaced with $n_y$ and $\mathcal{Y}$
respectively.

The object $\jac{f}{x}$ defines the following linear operations:
%
\begin{itemize}
%
{}\item \textbf{Jacobian forward operator-vector product}
%
\begin{eqnarray*}
u & = & \Jac{f}{x} v : \mathcal{X} \rightarrow \mathcal{F} \\
& \Rightarrow & \\
u_{(i)} & = & \sum_{j=1}^{n_x} \Jac{f_{(i)}}{x_{(j)}} v_{(j)}, \; i = 1 \ldots n_f,
\end{eqnarray*}
%
where $v\in\mathcal{X}$, and $u\in\mathcal{F}$.
%
{}\item \textbf{Jacobian forward operator-mutivector product}
%
\begin{eqnarray*}
U & = & \Jac{f}{x} V : \mathcal{X}|\mathcal{S} \rightarrow \mathcal{F}|\mathcal{S} \\
& \Rightarrow & \\
U_{(i,j)} & = & \sum_{k=1}^{n_x} \Jac{f_{(i)}}{x_{(k)}} V_{(k,j)}, \; i = 1 \ldots n_f, \; j = 1 \ldots n_s,
\end{eqnarray*}
%
where $V\in\mathcal{X}|\mathcal{S}$, and $V\in\mathcal{F}|\mathcal{S}$.
%
{}\item\textbf{Jacobian adjoint operator-vector product}
%
\begin{eqnarray*}
u & = & \Jac{f}{x}^T v : \mathcal{F} \rightarrow \mathcal{X} \\
& \Rightarrow & \\
u_{(i)} & = & \sum_{j=1}^{n_f} \Jac{f_{(j)}}{x_{(i)}} v_{(j)}, \; i = 1 \ldots n_x,
\end{eqnarray*}
%
where $v\in\mathcal{F}$, and $u\in\mathcal{X}$.
%
{}\item\textbf{Jacobian adjoint operator-multivector product}
%
\begin{eqnarray*}
U & = & \Jac{f}{x}^T V : \mathcal{F}|\mathcal{S} \rightarrow \mathcal{X}|\mathcal{S} \\
& \Rightarrow & \\
u_{(i,j)} & = & \sum_{k=1}^{n_x} \Jac{f_{(k)}}{x_{(i)}} v_{(k,j)}, \; i = 1 \ldots n_s, \; j = 1 \ldots n_f,
\end{eqnarray*}
%
where $V\in\mathcal{F}|\mathcal{S}$, and $U\in\mathcal{X}|\mathcal{S}$.
%
\end{itemize}

\subsubsection{Second-Order (Hessian) Operators}

We represent the second derivative operator of $f(x,y)$ evaluated at
some point $(x,y)$ as
%
\begin{equation}
\HessTwo{f}{x}{y}\in\mathcal{F}|\mathcal{X}|\mathcal{Y},
\end{equation}
%
which defines a three-dimensional linear object and in componentwise form is
%
\begin{equation}
\left(\Jac{f}{x}\right)_{(i,j,k)} = \Jac{f_{(i)}}{x_{(j)}}{y_{(k)}} \in\RE,
  \; i = 1 \ldots n_f, \; j = 1 \ldots n_y, \; , \; k = 1 \ldots n_x.
\end{equation}

In these second derivative computations, the partials $(\partial x {}\partial
y)$ may represent $(\partial x {}\partial x)$, $(\partial y {}\partial x)$,
$(\partial x {}\partial y)$ or $(\partial y {}\partial y)$.

For the general case $n_f\ge{}1$, this object defines the following
kinds of linear operations:
%
\begin{itemize}
%
{}\item \textbf{Adjoined Hessian forward operator-vector product}
%
\begin{eqnarray*}
u & = & w^T \HessTwo{f}{x}{y} v : \mathcal{Y} \rightarrow \mathcal{X} \\
& \Rightarrow & \\
u_{(i)} & = & \sum_{j=1}^{n_y} \sum_{k=1}^{n_f} w_{(k)}\HessTwo{f_{(k)}}{x_{(i)}}{y_{(j)}} v_{(j)},
   \; i = 1 \ldots n_x,
\end{eqnarray*}
%
where $v\in\mathcal{Y}$, $w\in\mathcal{F}$, and $u\in\mathcal{X}$.
%
{}\item \textbf{Adjoined Hessian forward operator-multivector product}
%
\begin{eqnarray*}
U & = & w^T \HessTwo{f}{x}{y} V : \mathcal{Y}|\mathcal{S} \rightarrow \mathcal{X}|\mathcal{S} \\
& \Rightarrow & \\
U_{(i,j)} & = & \sum_{k=1}^{n_y} \sum_{l=1}^{n_f} w_{(l)}\HessTwo{f_{(l)}}{x_{(i)}}{y_{(k)}} V_{(k,j)},
   \; i = 1 \ldots n_x,
\end{eqnarray*}
%
where $V\in\mathcal{Y}|\mathcal{S}$, $w\in\mathcal{F}$, and $U\in\mathcal{X}|\mathcal{S}$.
%
\end{itemize}

When $n_f=1$ then
%
\begin{equation}
\HessTwo{f}{x}{y}\in\mathcal{X}|\mathcal{Y},
\end{equation}
%
and the following linear operations are defined:
%
\begin{itemize}
%
{}\item \textbf{Hessian forward operator-vector product ($n_f=1$ only)}
%
\begin{eqnarray*}
u & = & \HessTwo{f}{x}{y} v : \mathcal{Y} \rightarrow \mathcal{X} \\
& \Rightarrow & \\
u_{(i)} & = & \sum_{j=1}^{n_y} \HessTwo{f}{x_{(i)}}{y_{(j)}} v_{(j)}, \; i = 1 \ldots n_x,
\end{eqnarray*}
%
where $v\in\mathcal{Y}$, and $u\in\mathcal{X}$.
%
{}\item \textbf{Hessian forward operator-multivector product ($n_f=1$ only)}
%
\begin{eqnarray*}
U & = & \HessTwo{f}{x}{y} V : \mathcal{Y}|\mathcal{S} \rightarrow \mathcal{X}|\mathcal{S} \\
& \Rightarrow & \\
U_{(i,j)} & = & \sum_{k=1}^{n_y} \HessTwo{f}{x_{(i)}}{y_{(k)}} V_{(k,j)}, \; i = 1 \ldots n_x,
\end{eqnarray*}
%
where $V\in\mathcal{Y}|\mathcal{S}$, and $U\in\mathcal{X}|\mathcal{S}$.
%
\end{itemize}

Note that for the $n_f=1$ case the adjoint object is defined as
%
\begin{equation}
\HessTwo{f}{x}{y}^T = \HessTwo{f}{y}{x}
\end{equation}
%
and the associated linear operations defined above directly follow.

\section{Derivation of Transient Sensitivity Computations}
\label{rythmos:app:sens-derivations}

Here we derive direct and adjoint sensitivity methods for general DAEs as
described in Section~\ref{rythmos:sec:trans-sens}.  Restated, the general DAE
model is
%
\begin{eqnarray}
c\left( \dot{y}(t), y(t), p, v(t), t \right) & = & 0,
\; t \in \left[ t_0, t_f \right] \label{rythmos:apdx:eqn:sens:c} \\
y(0) & = & y_0(p) \label{rythmos:apdx:eqn:sens:c:ic}
\end{eqnarray}
%
and the auxiliary response functions, for which sensitivities are computed, are
%
\begin{equation}
d(y,p,v)
= \int_{t_0}^{t_f} g(y(t),p,v(t),t) dt + h(y(t_f),p),
\label{rythmos:apdx:eqn:sens:d}
\end{equation}
%
where $y: t {}\rightarrow y(t){}\:\in\:\mathcal{Y}$ for $t\in[t_0,t_f]$,
$\dot{y}: t {}\rightarrow \dot{y}(t) = d(y)/d(t)\:\in\:\mathcal{Y}$ for
$t\in[t_0,t_f]$, $p{}\:\in\:\mathcal{P}$, $v: t {}\rightarrow
v(t){}\:\in\:\mathcal{V}$ for $t\in[t_0,t_f]$, $c(\dot{y}, y, p, v, t)
:{}\mathcal{Y}^2{}\times{}\mathcal{P}{}\times{}\mathcal{V}{}\times{}\RE
{}\rightarrow{}\mathcal{C}$,
$y_0(p){}\:\in\:\mathcal{P}{}\rightarrow{}\mathcal{Y}$, $g(y,p,v,t) :{}\:
{}\mathcal{Y}{}\times{}\mathcal{P}{}\times{}\mathcal{V}{}\times{}\RE
{}\rightarrow{}\mathcal{G}$, $h(y,p) :{}\: {}\mathcal{Y}{}\times{}\mathcal{P}
{}\rightarrow{}\mathcal{G}$, $\mathcal{G}{}\:\subseteq\:\RE^{n_g}$,
$\mathcal{Y}{}\:\subseteq\:\RE^{n_y}$, $\mathcal{P}{}\:\subseteq\:\RE^{n_p}$,
$\mathcal{V}{}\:\subseteq\:\RE^{n_v}$, and
$\mathcal{C}{}\:\subseteq\:\RE^{n_y}$.

{}\textit{Remarks on notation}: We use the commonly accepted convention where
the unadorned indentifier $y$ for a function such as $t {}\rightarrow y(t)$,
$t\in[t_0,t_f]$, is used to represent the function over the entire domain
$t\in[t_0,t_f]$.  When appropriate, the notation $y(t)$ will be used to
represent a particualar evaluation of the function at points $t$ in the domain
$[t_0,t_f]$.  However, in some situations, the argument $t$ of $y(t)$ will be
omitted and instead $y$ is used and it should be clear from the context (i.e.\
inside of an integral) that really a single value of the function is being
represented.  Cases of possible ambiguity will be addressed in the sequel.

The implicit state solution for the DAEs in
(\ref{rythmos:apdx:eqn:sens:c})--(\ref{rythmos:apdx:eqn:sens:c:ic}) at points
$t$ is signified as $y(p,v,t)\in\mathcal{Y}$, where $v$ in this case is the
selection of transient parameters in the range $t\in[t_0,t]$.  Note that
selections for $v$ in the range $(t:t_f]$ have no influence on the implicit
state solution $y$ up to the time $t$.  The full infinite-dimensional implicit
state solution for $y$ in $t\in[t_0,t_f]$ is signified as $y(p,v)$ where in
this case $v$ represents the entire selection for the transient parameters in
$[t_0,t_f]$.

Given the implicit state function $y(p,v)$, a reduced set of auxiliary response
functions are defined as
%
\begin{equation}
\hat{d}(p,v)
= \int_{t_0}^{t_f} \hat{g}(p,v(t),t) dt + \hat{h}(p),
\label{rythmos:apdx:eqn:sens:d_hat}
\end{equation}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$\hat{g}(p,v(t),t) = g(y(p,v,t),p,v(t),t)) : \:
		\mathcal{P} \times \mathcal{V} \times \RE \rightarrow \mathcal{G}$ is defined on $t\in[t_0,t_f]$, and \\
\>	$\hat{h}(p) = h(y(p,v,t_f),p)) : \:
		\mathcal{P} \rightarrow \mathcal{G}$ is defined only at $t=t_f$.
\end{tabbing}

\subsection{Derivation of direct sensitivities}
\label{rythmos:app:direct-sens-derivation}

For the derivation of direct sensitivities we will ignore the
transient parameters $v(t)$ for $t\in[t_0,t_f]$ since direct
sensitivity methods for such problems are typically impractical.

Note: For some applications where $n_v$ is not too large and where
$v$ is given a very course discretization in $t\in[t_0,t_f]$,
computing direct sensitivities with respect to the discretized $v$ can
be practical.

The direct sensitivity problem is easily stated by differentiating
(\ref{rythmos:apdx:eqn:sens:d_hat}) with respect to $p$ to obtain
%
\begin{equation}
\Jac{\hat{d}}{p}
= \int_{t_0}^{t_f} \left( \Jac{g}{y} \Jac{y}{p} +  \Jac{g}{p} \right) dt
+ \left. \left(  \Jac{h}{y} \Jac{y}{p} + \Jac{h}{p} \right) \right|_{t=t_f},
\label{rythmos:apdx:eqn:sens:d_d_hat_d_p}
\end{equation}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$\jac{\hat{g}}{p} \in \mathcal{G}|\mathcal{P}$ is defined on $t\in[t_0,t_f]$, \\
\>	$\jac{\hat{h}}{p} \in \mathcal{G}|\mathcal{P}$ is defined only at $t=t_f$, \\
\>	$\jac{y}{p} \in \mathcal{Y}|\mathcal{P}$ is the direct sensitivity of the state defined on $t\in[t_0,t_f]$, and \\
\>	$\jac{\hat{d}}{p} \in \mathcal{G}|\mathcal{P}$ is defined independent of time.
\end{tabbing}

The direct sensitivity $\jac{y}{p}$ in
(\ref{rythmos:apdx:eqn:sens:d_d_hat_d_p}) is computed by solving a set of
$n_p$ independent direct sensitivity equations which are obtained by
differentiating
(\ref{rythmos:apdx:eqn:sens:c})--(\ref{rythmos:apdx:eqn:sens:c:ic}) with
respect to $p$ to give
%
\begin{eqnarray}
%
\Jac{c}{\dot{y}} \left(\frac{d}{dt} \Jac{y}{p} \right) + \Jac{c}{y} \left(\Jac{y}{p}\right)
+ \Jac{c}{p} & = & 0, \; t \in \left[ t_0, t_f \right], \label{rythmos:apdx:eqn:sens:direct-c} \\
\Jac{y(t_0)}{p} & = & \Jac{y_0}{p}, \label{rythmos:apdx:eqn:sens:direct-c:ic}
\end{eqnarray}
\begin{tabbing}
\hspace{4ex}where\hspace{1ex}\= \\
\>	$\jac{c}{\dot{y}} \in \mathcal{C}|\mathcal{Y}$ is defined on $t\in[t_0,t_f]$, \\
\>	$\jac{c}{y} \in \mathcal{C}|\mathcal{Y}$ is defined on $t\in[t_0,t_f]$, \\
\>	$\jac{c}{p} \in \mathcal{C}|\mathcal{P}$ is defined on $t\in[t_0,t_f]$, and \\
\>	$\jac{y_0}{p} \in \mathcal{Y}|\mathcal{P}$ is defined only at $t=t_0$.
\end{tabbing}

These $n_p$ independent sensitivity equations are solved for $\jac{y}{p}
{}\in\mathcal{Y}|\mathcal{P}$ forward in time from $t_0$ to $t_f$.  The
integral in (\ref{rythmos:apdx:eqn:sens:d_d_hat_d_p}) can be evaluated along
with the integration of
(\ref{rythmos:apdx:eqn:sens:c})--(\ref{rythmos:apdx:eqn:sens:c:ic}) and
(\ref{rythmos:apdx:eqn:sens:direct-c})--(\ref{rythmos:apdx:eqn:sens:direct-c:ic}).
Note that (\ref{rythmos:apdx:eqn:sens:c})--(\ref{rythmos:apdx:eqn:sens:c:ic})
and
(\ref{rythmos:apdx:eqn:sens:direct-c})--(\ref{rythmos:apdx:eqn:sens:direct-c:ic})
are a staggered set of DAE equations in that
(\ref{rythmos:apdx:eqn:sens:c})--(\ref{rythmos:apdx:eqn:sens:c:ic}) are solved
first followed by
(\ref{rythmos:apdx:eqn:sens:direct-c})--(\ref{rythmos:apdx:eqn:sens:direct-c:ic}).

\subsection{Derivation of adjoint sensitivities}

Here we describe a derivation for the adjoint equation and the gradient
expressions that are based on simple and straightforward principles.

\subsubsection{Derivation of adjoint equation and sensitivities for steady-state parameters}
\label{rythmos:app:adj-equ-derivation}

In this section we begin with the same basic expressions for the reduced
derivative $\jac{\hat{d}}{p}$ in (\ref{rythmos:apdx:eqn:sens:d_d_hat_d_p}) and
the direct sensitivity equations for $\jac{y}{p}$ in
(\ref{rythmos:apdx:eqn:sens:direct-c})--(\ref{rythmos:apdx:eqn:sens:direct-c:ic})
and then perform basic manipulations in order to arrive at the adjoint
equations and the adjoint sensitivities for $\jac{\hat{d}}{p}$.

We begin our derivation of the adjoint equation and sensitivities for
steady-state parameters by writing the weak form of the direct sensitivity
equation (\ref{rythmos:apdx:eqn:sens:direct-c}) which is
%
\begin{equation}
\int_{t_0}^{t_f} \Lambda^T \left(
\Jac{c}{\dot{y}} \left(\frac{d}{dt} \Jac{y}{p} \right)
+ \Jac{c}{y} \left(\Jac{y}{p}\right)
+ \Jac{c}{p}
\right) dt = 0,
\label{rythmos:apdx:eqn:sens:weak-direct-c}
\end{equation}
%
where at this point the function $\Lambda$, for
$t\rightarrow\Lambda(t)\in\mathcal{C}|\mathcal{G}$ in $t\in[t_0,t_f]$, is any
appropriate weighting function\footnote{The admissible class of wieghting
functions $\Lambda$ are those functions that are sufficiently smooth such that
the integrals of the weak form (and the modified weak form after integration
by parts) are finite and well defined.  Other requirements may also need to be
satisfied in some cases [???].}.  Later, $\Lambda$ will be chosen to be the
adjoint variables but for now $\Lambda$ is nothing more than an arbitrary
weighting function for the purpose of stating the weak form.  The solution to
the weak form of (\ref{rythmos:apdx:eqn:sens:weak-direct-c}) is every bit as
valid and is indeed more general than the strong form in
(\ref{rythmos:apdx:eqn:sens:direct-c}) and we lose nothing by considering the
weak form {}\cite{BeckerCareyOden-FE}.

Next, we substitute the integration by parts
%
\begin{equation}
\int_{t_0}^{t_f} \left[ \left( \Lambda^T \Jac{c}{\dot{y}} \right) \frac{d}{dt}\left( \Jac{y}{p} \right) \right] dt
= \left. \left( \Lambda^T \Jac{c}{\dot{y}} \Jac{y}{p} \right) \right|_{t_0}^{t_f}
- \int_{t_0}^{t_f} \left[ \frac{d}{dt}\left( \Lambda^T \Jac{c}{\dot{y}} \right) \Jac{y}{p} \right] dt
\end{equation}
%
into (\ref{rythmos:apdx:eqn:sens:weak-direct-c}) and rearrange which yields
%
\begin{equation}
\int_{t_0}^{t_f} \left( \Lambda^T \Jac{c}{p} \right) dt
+ \int_{t_0}^{t_f} \left[
    - \frac{d}{dt}\left( \Lambda^T \Jac{c}{\dot{y}} \right)
    + \Lambda^T \Jac{c}{y}
  \right] \Jac{y}{p} dt
+ \left. \left( \Lambda^T \Jac{c}{\dot{y}} \Jac{y}{p} \right) \right|_{t_0}^{t_f}
= 0.
\label{rythmos:apdx:eqn:sens:weak-direct-c-2}
\end{equation}
%
At this point in the derivation we decide to restrict the possible set of
functions for $\Lambda$ by forcing $\Lambda$ to satisfy the differential
equation
%
\begin{equation}
- \frac{d}{dt}\left( \Lambda^T \Jac{c}{\dot{y}} \right)
+  \Lambda^T \Jac{c}{y} = - \Jac{g}{y}, \; t \in \left[ t_0, t_f \right].
\label{rythmos:apdx:eqn:sens:adj-trans}
\end{equation}
%
All that we have done in (\ref{rythmos:apdx:eqn:sens:adj-trans}) is to make
what appears to be an arbitrary choice to help narrow the weighting function
$\Lambda$ from the infinite set of possible choices.  It will be clear below
why this chose is a convenient one.  Note that the choice in
(\ref{rythmos:apdx:eqn:sens:adj-trans}) does not in and of itself uniquely
determine $\Lambda$ as boundary conditions have yet to be specified.  It will
be shown latter that the choice of boundary condition is arbitrary but when we
move toward the end of the derivation, a natural choice for a boundary
condition that uniquely specifies $\Lambda$ will fall out.

The derivation continues by substituting
(\ref{rythmos:apdx:eqn:sens:adj-trans}) into
(\ref{rythmos:apdx:eqn:sens:weak-direct-c-2}) and rearranging which yields
%
\begin{eqnarray}
\int_{t_0}^{t_f} \left( \Jac{g}{y} \Jac{y}{p} \right) dt
& = &  \int_{t_0}^{t_f} \left( \Lambda^T \Jac{c}{p} \right) dt
+ \left. \left( \Lambda^T \Jac{c}{\dot{y}} \Jac{y}{p} \right) \right|^{t_f}_{t_0}.
\label{rythmos:apdx:eqn:sens:weak-direct-c-3}
\end{eqnarray}

Finally, substituting (\ref{rythmos:apdx:eqn:sens:weak-direct-c-3}) into
(\ref{rythmos:apdx:eqn:sens:d_d_hat_d_p}) and rearranging gives
%
\begin{eqnarray}
\Jac{\hat{d}}{p} 
& = & \int_{t_0}^{t_f} \left( \Lambda^T \Jac{c}{p} + \Jac{g}{p} \right) dt
  + \left. \left( \Jac{h}{p} \right) \right|_{t=t_f}
  - \left. \left( \Lambda^T \Jac{c}{\dot{y}} \Jac{y}{p} \right) \right|_{t=t_0}
\nonumber \\
& & + \left. \left[ \left(
    \Lambda^T \Jac{c}{\dot{y}}
    +  \Jac{h}{y}
  \right)  \Jac{y}{p} \right] \right|_{t=t_f}.
\label{rythmos:apdx:eqn:sens:d_d_hat_d_p-3}
\end{eqnarray}
%
We are not finished yet since, as we stated earlier, the choice for the
weighting functions $\Lambda$ have not yet been uniquely specified.  The first
thing to consider is that, in general, the adjoint DAE in
(\ref{rythmos:apdx:eqn:sens:adj-trans}) is only stable if integrated backwards
in time from $t_f$ to $t_0$ {}\cite{adjoint-sens-2003}.  However, as described
in the following theorem, we are free to pick almost any final condition for
$\Lambda(t_f)$ that is consistent with the adjoint equation at $t_f$ and then
evaluate the terms in (\ref{rythmos:apdx:eqn:sens:d_d_hat_d_p-3}) involving
the resulting adjoint solution.

\begin{theorem}
If $\Lambda_1$ and $\Lambda_2$ are any two particular bounded solutions to the
adjoint equation (\ref{rythmos:apdx:eqn:sens:adj-trans}) (each associated with
a different bounded consistent final condition on $\Lambda(t_f)$) then
%
\begin{equation}
\left.\Jac{\hat{d}}{p}\right|_{\Lambda=\Lambda_1} = \left.\Jac{\hat{d}}{p}\right|_{\Lambda=\Lambda_2}
\label{rythmos:apdx:eqn:sens:d_d_hat_d_p-same}
\end{equation}
%
where $\left.(\jac{\hat{d}}{p})\right|_{\Lambda=\Lambda_1}$ and
$\left.(\jac{\hat{d}}{p})\right|_{\Lambda=\Lambda_1}$ represent the reduced
derivative in (\ref{rythmos:apdx:eqn:sens:d_d_hat_d_p-3}) evaluated using the
adjoint solutions $\Lambda_1$ and $\Lambda_2$ respectively.
\end{theorem}

\textbf{Proof}

Here we prove (\ref{rythmos:apdx:eqn:sens:d_d_hat_d_p-same}) by proving that
%
\begin{eqnarray}
D_2 - D_1
& = &
\left.\Jac{\hat{d}}{p}\right|_{\Lambda=\Lambda_2} - \left.\Jac{\hat{d}}{p}\right|_{\Lambda=\Lambda_1}
\nonumber \\
& = &
\int_{t_0}^{t_f} \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{c}{p} dt
+ \left.\left[ \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{c}{\dot{y}} \Jac{y}{p} \right]\right|_{t_0}^{t_f}
= 0
\label{rythmos:apdx:eqn:sens:ta:d_hat_d_d_p-diff-zero}
\end{eqnarray}
%
for any two particular solutions $\Lambda_1$ and $\Lambda_2$ to the adjoint
equation (\ref{rythmos:apdx:eqn:sens:adj-trans}).

We start the proof by substituting (\ref{rythmos:apdx:eqn:sens:direct-c}) into
(\ref{rythmos:apdx:eqn:sens:ta:d_hat_d_d_p-diff-zero}) which yields
%
\begin{equation}
D_2 - D_1 = 
\int_{t_0}^{t_f} \left( \Lambda_2 - \Lambda_1 \right)^T \left[
  - \Jac{c}{\dot{y}} \left(\frac{d}{dt} \Jac{y}{p} \right) - \Jac{c}{y} \left(\Jac{y}{p}\right) \right] dt
+ \left.\left[ \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{c}{\dot{y}} \Jac{y}{p} \right]\right|_{t_0}^{t_f}.
\label{rythmos:apdx:eqn:sens:ta:d_hat_d_d_p-diff-zero-2}
\end{equation}
%
Substituting the integration by parts
%
\[
- \int_{t_0}^{t_f} \left[ \left( \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{c}{\dot{y}} \right) \frac{d}{dt}\left( \Jac{y}{p} \right) \right] dt
= \int_{t_0}^{t_f} \left[ \frac{d}{dt}\left( \left( \Lambda_2 - \Lambda_1 \right)^T  \Jac{c}{\dot{y}} \right) \Jac{y}{p} \right] dt
- \left. \left[ \left( \Lambda_2 - \Lambda_1 \right)^T  \Jac{c}{\dot{y}} \Jac{y}{p} \right] \right|_{t_0}^{t_f}
\]
%
into (\ref{rythmos:apdx:eqn:sens:ta:d_hat_d_d_p-diff-zero-2}), canceling
terms, and rearranging yields
%
\begin{equation}
D_2 - D_1 = 
\int_{t_0}^{t_f} \left[ \frac{d}{dt} \left( \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{c}{\dot{y}} \right)
- \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{c}{y} \right] \left(\Jac{y}{p}\right) dt.
\label{rythmos:apdx:eqn:sens:ta:d_hat_d_d_p-diff-zero-3}
\end{equation}
%
Next, substituting the solutions $\Lambda_1$ and $\Lambda_2$ into
(\ref{rythmos:apdx:eqn:sens:adj-trans}) and substracting these two adjoint
equations yields
%
\begin{equation}
- \frac{d}{dt} \left( \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{c}{\dot{y}} \right)
+ \left( \Lambda_2 - \Lambda_1 \right)^T \Jac{c}{y} = 0, \; t \in [t_0,t_f].
\label{rythmos:apdx:eqn:sens:ta:adj-trans-diff}
\end{equation}
%
Finally, substituting (\ref{rythmos:apdx:eqn:sens:ta:adj-trans-diff}) into
(\ref{rythmos:apdx:eqn:sens:ta:d_hat_d_d_p-diff-zero-3}) yeilds
%
\begin{equation}
D_2 - D_1 = 0
\label{rythmos:apdx:eqn:sens:ta:d_hat_d_d_p-diff-zero-final}
\end{equation}
%
which completes the proof. $\;\Box$

The above theorem establishes that there are an infinite number of choices for
the final condition for the adjoint solution that all yeild the same reduced
derivative $\jac{\hat{d}}{p}$.  While a wide variety of final conditions for
$\Lambda(t_f)$ can be chosen to close the adjoint equation, rather than
picking just any final condition, selecting the final condition
%
\begin{equation}
\left. \left(
  \Lambda^T \Jac{c}{\dot{y}}
  +  \Jac{h}{y}
\right) \right|_{t=t_f}
 = 0,
\label{rythmos:apdx:eqn:sens:adj-trans:fc}
\end{equation}
%
is rather convenient since it zeros out the last term in
(\ref{rythmos:apdx:eqn:sens:d_d_hat_d_p-3}).  This specification of the final
condition closes the adjoint equation and uniquely defines the adjoint
$\Lambda$ in all of the cases of state DAEs what we will condider
here\footnote{A uniquely defined adjoint $\Lambda$ requires that certain
regularity conditions are satisfied by the state DAE (see [???]) but for the
purposes of this derivation we assume that such a condition is always
statisfied.}.  

Finally, substituting (\ref{rythmos:apdx:eqn:sens:adj-trans:fc}) and
$\jac{y_0}{p}$ for $\jac{y}{p}$ at $t=t_0$ in
(\ref{rythmos:apdx:eqn:sens:d_d_hat_d_p-3}) we arrive the final expression for
reduced derivative
%
\begin{equation}
\Jac{\hat{d}}{p} =
\int_{t_0}^{t_f} \left(
    \Jac{g}{p}
    + \Lambda^T \Jac{c}{p}
  \right) dt
  + \left. \left( \Jac{h}{p} \right) \right|_{t=t_f}
  - \left. \left( \Lambda^T \Jac{c}{\dot{y}} \Jac{y_0}{p} \right) \right|_{t=t_0}.
\label{rythmos:apdx:eqn:sens:d_d_hat_d_p_final}
\end{equation}

In conclusion, (\ref{rythmos:apdx:eqn:sens:adj-trans}) and
(\ref{rythmos:apdx:eqn:sens:adj-trans:fc}) define the set of adjoint DAE
equations that solve for a unique adjoint solution $\Lambda$ which are
integrated backwards in time and
(\ref{rythmos:apdx:eqn:sens:d_d_hat_d_p_final}) gives the expression for the
computation of $\jac{\hat{d}}{p}$ in terms of the computed adjoint $\Lambda$.

\subsubsection{Derivation of adjoint sensitivities for transient parameters}

Here we consider the derivation of the reduced sensitivity with respect to
transient parameters
%
\begin{equation}
\Jac{\hat{d}}{v(t)} \in \mathcal{G}|\mathcal{V}, \; \mbox{for} \; t\in[t_0,t_f].
\end{equation}
%
Our derivation for these sensitivities for the transient parameters $v$ will
be a little different than for the steady-state parameters $p$.  The primary
reason for using a different derivation is that the infinite-dimensional
transient parameters $v$ are fundamentally different than finite-dimensional
steady-state parameters $p$.  Because of the infinite-dimensional nature of
$v$, we use a different tool of functional analysis.  The tool we use is the
application of the infinite-dimensional operator $\jac{\hat{d}}{v}$ to the
infinite-dimensional perturbation $\delta v$ where we will then use identify
$\jac{\hat{d}}{v(t)}$ in the integrand of
%
\begin{equation}
\Jac{\hat{d}}{v} \delta v = \int_{t_0}^{t_f} \Jac{\hat{d}}{v(t)} \delta v(t) dt.
\label{rythmos:apdx:eqn:sens:d_d_hat_d_v_delta_v}
\end{equation}
%
The integral in (\ref{rythmos:apdx:eqn:sens:d_d_hat_d_v_delta_v}) is the
definition the infinite-dimensional operator application
$(\jac{\hat{d}}{v(t)}) {}\delta v$ [???].

We begin the derivation with the directional derivative of $\hat{d}(v)$ with
respect to $v$ along the direction $\delta v$ which is simply
%
\begin{equation}
\Jac{\hat{d}}{v} \delta v
= \int_{t_0}^{t_f} \left( \Jac{g}{y} \Jac{y}{v} \delta v +  \Jac{g}{v} \delta v \right) dt
+ \left. \left(  \Jac{h}{y} \Jac{y}{v} \delta_v \right) \right|_{t=t_f}.
\label{rythmos:apdx:eqn:sens:d_d_hat_d_v_delta_v-2}
\end{equation}
%
Next, we write the weak form of the linearization of the state equation (about
a solution for $c(\dot{y},y,p,v,t)=0$) with respect to $v$ along the direction
$\delta v$ which is
%
\begin{equation}
%
\int_{t_0}^{t_f} \Lambda^T \left[
\Jac{c}{\dot{y}} \left(\frac{d}{dt} \Jac{y}{v} \right) \delta v + \Jac{c}{y} \Jac{y}{v} \delta v
+ \Jac{c}{v} \delta_v \right] dt = 0
\label{rythmos:apdx:eqn:sens:weak-direct-c-v}
\end{equation}
%
and has initial condition $\left.(\jac{y}{v})\delta v\right|_{t=t_0} = 0$.
The sensitivity equation that is used in the weak form in
(\ref{rythmos:apdx:eqn:sens:weak-direct-c-v}) can be derived using a simple
Taylor expansion of the state equation about a solution $c(\dot{y},y,p,v,t)=0$
in the direction $\delta v$ and then dropping off the $O(||\delta v||^2)$ and
higher terms.

From here the derivation proceeds almost identically to the case for the
steady-state parameters $p$ described in the previous section.

Substituting the integration by parts
%
\begin{equation}
\int_{t_0}^{t_f} \left[ \left( \Lambda^T \Jac{c}{\dot{y}} \right) \frac{d}{dt}\left( \Jac{y}{v} \delta v \right) \right] dt
= \left. \left( \Lambda^T \Jac{c}{\dot{y}} \Jac{y}{v} \delta v \right) \right|_{t_0}^{t_f}
- \int_{t_0}^{t_f} \left[ \frac{d}{dt}\left( \Lambda^T \Jac{c}{\dot{y}} \right) \Jac{y}{v} \delta v \right] dt
\end{equation}
%
into (\ref{rythmos:apdx:eqn:sens:weak-direct-c-v}) and rearranging yeilds
%
\begin{equation}
\int_{t_0}^{t_f} \left( \Lambda^T \Jac{c}{v} \delta v \right) dt
+ \int_{t_0}^{t_f} \left[
    - \frac{d}{dt}\left( \Lambda^T \Jac{c}{\dot{y}} \right)
    + \Lambda^T \Jac{c}{y}
  \right] \Jac{y}{v} \delta v dt
+ \left. \left( \Lambda^T \Jac{c}{\dot{y}} \Jac{y}{v} \delta v \right) \right|_{t_0}^{t_f}
= 0.
\label{rythmos:apdx:eqn:sens:weak-direct-c-v-2}
\end{equation}
%
Substituting the choice for the adjoint equation
(\ref{rythmos:apdx:eqn:sens:adj-trans}) into
(\ref{rythmos:apdx:eqn:sens:weak-direct-c-v-2}) and rearranging yeilds
%
\begin{eqnarray}
\int_{t_0}^{t_f} \left( \Jac{g}{y} \Jac{y}{v} \delta v \right) dt
& = &  \int_{t_0}^{t_f} \left( \Lambda^T \Jac{c}{v} \delta v \right) dt
+ \left. \left( \Lambda^T \Jac{c}{\dot{y}} \Jac{y}{v} \delta v \right) \right|^{t_f}_{t_0}.
\label{rythmos:apdx:eqn:sens:weak-direct-c-v-3}
\end{eqnarray}
%
Substituting (\ref{rythmos:apdx:eqn:sens:weak-direct-c-v-3}) into
(\ref{rythmos:apdx:eqn:sens:d_d_hat_d_v_delta_v-2}) and rearranging
yeilds
%
\begin{eqnarray}
\Jac{\hat{d}}{v} \delta v 
& = & \int_{t_0}^{t_f} \left( \Lambda^T \Jac{c}{v} + \Jac{g}{v} \right) \delta v dt
  - \left. \left( \Lambda^T \Jac{c}{\dot{y}} \Jac{y}{v} \delta v \right) \right|_{t=t_0}
\nonumber \\
& & + \left. \left[ \left(
    \Lambda^T \Jac{c}{\dot{y}} +  \Jac{h}{y}
  \right)  \Jac{y}{v} \delta v \right] \right|_{t=t_f}.
\label{rythmos:apdx:eqn:sens:d_d_hat_d_v_delta_v-3}
\end{eqnarray}
%
Using the choice for the final condition in
(\ref{rythmos:apdx:eqn:sens:adj-trans:fc}) and noting that
$\left.(\jac{y}{v})\delta v\right|_{t=t_0} = 0$, then
(\ref{rythmos:apdx:eqn:sens:d_d_hat_d_v_delta_v-3}) reduces to
%
\begin{equation}
\Jac{\hat{d}}{v} \delta v 
 = \int_{t_0}^{t_f} \left( \Lambda^T \Jac{c}{v} + \Jac{g}{v} \right) \delta v dt.
\label{rythmos:apdx:eqn:sens:d_d_hat_d_v_delta_v-final}
\end{equation}

Finally, by comparing (\ref{rythmos:apdx:eqn:sens:d_d_hat_d_v_delta_v-final})
with (\ref{rythmos:apdx:eqn:sens:d_d_hat_d_v_delta_v}) it is clear that
%
\begin{equation}
\Jac{\hat{d}}{v(t)} = \Lambda(t)^T \Jac{c}{v(t)} + \Jac{g}{v(t)}, \; t\in[t_0,t_f]
\label{rythmos:apdx:eqn:sens:d_d_hat_d_v_t-final}
\end{equation}
%
is the reduced derivative object that we are seeking.

\subsection{Significance and interpretation of the adjoint}

In this section we consider another reason for choosing
(\ref{rythmos:apdx:eqn:sens:adj-trans:fc}) as the final condition to close the
adjoint equation (\ref{rythmos:apdx:eqn:sens:adj-trans}).  This choice of the
final condition makes the adjoint variables $\Lambda$ become the first-order
sensitivity of the auxiliary response function $d(y)$ with respect the
perturbations in the constraints through the state solution.  This is easy to
see by simply considering the case where $g(y,v,t) = g(y)$, $h(y) = h(y)$, and
$c(\dot{y},y,v,t) = c(\dot{y},y,t) + v$ which, when substituted into
(\ref{rythmos:apdx:eqn:sens:d_d_hat_d_v_t-final}), gives
%
\begin{equation}
\Jac{\hat{d}}{v(t)} = \Lambda(t)^T , \; t\in[t_0,t_f],
\label{rythmos:apdx:eqn:sens:adjoint_interpretation}
\end{equation}
%
where here $v(t)\in\mathcal{C}$ are the perturbations in the state
constraints.

The knowledge that this selection for the adjoint $\Lambda$ is the sensitivity
of a functional of interest $d(y)$ with respect the changes in the constriants
makes the adjoint a useful quantitiy in and of itself.  This is why the
adjoint is useful in more contexts other than just computing reduced
deriatives.  Examples of other areas where adjoints are useful are sensitivity
analysis [???] and error estimation [???].

\subsection{The adjoint, the reduced gradient and the augmented adjoint}

We now restate the adjoint in (\ref{rythmos:apdx:eqn:sens:adj-trans}) and
(\ref{rythmos:apdx:eqn:sens:adj-trans:fc}), and the reduced sensitivities in
(\ref{rythmos:apdx:eqn:sens:d_d_hat_d_p_final}) and
(\ref{rythmos:apdx:eqn:sens:d_d_hat_d_v_t-final}) in standard column-wise form
for $\Lambda$ which yields the adjoint DAE equations
%
\begin{eqnarray}
- \frac{d}{dt}\left( \Jac{c}{\dot{y}}^T \Lambda \right)
+  \Jac{c}{y}^T \Lambda + \Jac{g}{y}^T
& = & 0, \; t \in \left[ t_0, t_f \right],
\label{rythmos:apdx:eqn:sens:adj} \\
\left.\left( \Jac{c}{\dot{y}}^T \Lambda \right)\right|_{t=t_f}
& = & - \left. \Jac{h}{y}^T \right|_{y=y(t_f)},
\label{rythmos:apdx:eqn:sens:adj:fc}
\end{eqnarray}
%
and the reduced gradient expressions
%
\begin{equation}
\Jac{\hat{d}}{p}^T =
\int_{t_0}^{t_f} \left(
    \Jac{g}{p}^T
    + \Jac{c}{p}^T \Lambda
  \right) dt
  + \left. \Jac{h}{p}^T \right|_{t=t_f}
  - \left. \left( \Jac{y_0}{p}^T \Jac{c}{\dot{y}}^T \Lambda \right) \right|_{t=t_0}.
\label{rythmos:apdx:eqn:sens:d_d_hat_d_p_final-2}
\end{equation}
%
and
%
\begin{equation}
\Jac{\hat{d}}{v(t)}^T =  \Jac{g}{v(t)}^T + \Jac{c}{v(t)}^T \Lambda(t), \; t\in[t_0,t_f]
\label{rythmos:apdx:eqn:sens:d_d_hat_d_v_t-final-2}
\end{equation}
%

As described in {}\cite{adjoint-sens-2003}, for some classes of up to index-2
DAEs (including implicit ODES), the form of the adjoint in
(\ref{rythmos:apdx:eqn:sens:adj})--(\ref{rythmos:apdx:eqn:sens:adj:fc}) may be
unstable while the {}\textit{augmented adjoint system}
%
\begin{eqnarray}
\frac{d}{dt}\left( \bar{\Lambda} \right)
+  \Jac{c}{y}^T \Lambda + \Jac{g}{y}^T
& = & 0, \; t \in \left[ t_0, t_f \right],
\label{rythmos:apdx:eqn:sens:aug-adj-de} \\
\bar{\Lambda} + \Jac{c}{\dot{y}}^T \Lambda
& = & 0, \; t \in \left[ t_0, t_f \right],
\label{rythmos:apdx:eqn:sens:aug-adj-ae} \\
\left. \bar{\Lambda} \right|_{t=t_f}
& = & \left. \Jac{h}{y}^T \right|_{y=y(t_f)},
\label{rythmos:apdx:eqn:sens:aug-adj:fc}
\end{eqnarray}
%
generally is stable when the forward DAE is stable.  In addition, when
considering timestepping algorithms for state and adjoint equations, only the
augmented adjoint DAE is guaranteed to be stable for the same timestep used by
the forward DAE {}\cite{adjoint-sens-2003}.  Therefore, the augmented adjoint
DAE system is to be preferred in a software implementation.

\section{ToDo}

\begin{itemize}
%
{}\item Ross: Include requirements for spatially continuous direct and adjoint
sensitivity methods and describe how these can easily be exploited (even when
these sensitivities live on different spaces using different spatial
discretizations).
%
{}\item Ross: Include requirements for explicit time integrators for direct
and adjoint sensitivitity systems.  Describe how AD can easily be used to
provide such evaluations.  Just because an application can not use an implicit
time integration method is no excuse for not supporting direct and adjoint
sensitivity methods, especially if continuous sensitivities are to be
considered.
%
{}\item Ross: Derive reduced Hessian-vector products for sensitivity
computations and expand the list of requirements to include second derivative
computations.  This is needed for the time domain decomposition work.
%
{}\item Ross: Get formulations for stability and eigen problems and make sure
requirements are covered.
%
{}\item Ross: Write out adjoint formulations for the basic forward
time integration methods (do this once we start actually implementing
adjoint solvers) such that result in discrete adjoints in time.  Idea,
record time steps of forward calculation to reuse in adjoint
calculation if discrete adjoints in time are really needed.
%
{}\item Ross: Work out more carefully what the component forms of the
linear operators in Appendix A are.
%
%{}\item Ross: Look into issue of the use of scalar products and implicit ODEs that
%avoids explicit inverses of the mass matrix... RAB, I asked Scott Collis about this
%and there is no trick here.  For DG methods, it is easy to come up with a formulation
%that gives rise to an identity for the mass matrix.  For general FE methods, you
%have to actually invert the mass matrix to write an explicit ODE solver and therefore
%implicit methods are usually more attractive.
%
{}\item Ross or Todd: Add formulation for some popular explicit time stepping
methods.
%
% {}\item Todd: Add formulation for implicit ``generalized alpha'' methods. Done, 2/23/2005
%
% {}\item Todd: Add formulation for implicit ``theta'' methods. Done, 2/24/2004
%
{}\item Ross or Todd: Add the ODE formulation for Premo and what transient (or
pseudo-transient) calculations that Premo performs.
%
{}\item Ross: Ask Kevin how Sundance might be able to support all of these
requirements just by specifying the weak form of the state equation and by
specifying distributed and terminal response functions.  Sundance should be
able to produce all of the discretized derivative objects automatically.
%
{}\item Ross: Add a discussion of parareal and how it can speed up the
computation and how it can be supported by general time integrators.
%
{}\item Ross: Add sections on linearized state, adjoint and gradient
computations and describe how to use them to compute Hessian-vector products.
Also, show the relationship between the linearized state equation and the
adjoint equation and how to test one against the other.
%
{}\item Ross: Add a section on a general formulations for state, linearized
state and adjoint DAEs that are amiable to direct software implementations.
%
\end{itemize}

\begin{SANDdistribution}
% External
% Housekeeping copies necessary for every unclassified report:
\SANDdistInternal{1}{9018}{Central Technical Files}{8945-1}
\SANDdistInternal{2}{0899}{Technical Library}{9610}
\SANDdistInternal{2}{0612}{Review \& Approval Desk}{4916}
% If report has a Patent Caution or Patent Interest, add this:
%\SANDdistInternal{3}{0161}{Patent and Licensing Office}{4916}
\end{SANDdistribution}

\end{document}
