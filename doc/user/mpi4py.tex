
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copyright (c) 2025 by The University of Queensland
% http://www.uq.edu.au
%
% Primary Business: Queensland, Australia
% Licensed under the Apache License, version 2.0
% http://www.apache.org/licenses/LICENSE-2.0
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%!TEX root = user.tex
\chapter{Using \mpifo with \escript}
\label{CHAP:mpi4py}

\section{Introduction}

\mpifo\footnote{\url{https://mpi4py.readthedocs.io/}} is a Python package that provides bindings to the Message Passing Interface (\MPI) standard.
When \escript is built with \mpifo support enabled, you can use standard \MPI programming techniques directly in your \PYTHON scripts to control how \escript uses parallel resources.

This chapter explains how to use \mpifo to:
\begin{itemize}
\item Pass custom \MPI communicators to \escript domains
\item Run multiple simulations simultaneously on different process groups
\item Implement ensemble simulations and parameter sweeps
\item Couple multiple physics domains on separate communicators
\item Retrieve \MPI communicators from \escript objects for custom communication
\end{itemize}

\subsection{Why use \mpifo with \escript?}

By default, when \escript runs with multiple \MPI processes, it automatically uses all available processes to solve each PDE.
This works well for large, high-resolution problems.
However, there are scenarios where you might want more control:

\begin{enumerate}
\item \textbf{Ensemble simulations}: Run multiple parameter values simultaneously, each on a smaller group of processes
\item \textbf{Parameter sweeps}: Explore parameter space by running many simulations in parallel
\item \textbf{Multi-physics coupling}: Solve different physics on different process groups
\item \textbf{Uncertainty quantification}: Run multiple realizations of stochastic problems
\item \textbf{Optimization}: Evaluate multiple trial solutions in parallel
\end{enumerate}

Using \mpifo gives you direct control over how processes are organized and allows you to implement these patterns using standard \MPI techniques.

\subsection{Requirements}

To use \mpifo with \escript:
\begin{itemize}
\item \escript must be built with \texttt{mpi4py=True} in the build configuration
\item \mpifo must be installed and compiled against the same \MPI implementation as \escript
\item Use \texttt{mpi='auto'} in the build configuration to automatically detect the correct \MPI implementation
\end{itemize}

See the \escript installation guide for detailed instructions.

\subsection{MPI Initialization}
\label{sec:mpi4py:init}

\textbf{Important}: When running \PYTHON scripts directly (not using the \texttt{run-escript} launcher), you \textbf{must} import \mpifo before importing any \escript modules to ensure \MPI is properly initialized.

\subsubsection{Correct Import Order}

Always import \mpifo \textbf{before} importing \escript modules:

\begin{python}
# CORRECT - Import mpi4py first
from mpi4py import MPI
from esys.ripley import Rectangle

domain = Rectangle(n0=100, n1=100)
\end{python}

The import statement \texttt{from mpi4py import MPI} automatically initializes \MPI when the module is loaded.
This must happen before any \escript domains are created.

\subsubsection{Common Mistake}

Do \textbf{not} import \escript modules before \mpifo:

\begin{python}
# WRONG - This will fail!
from esys.ripley import Rectangle
from mpi4py import MPI  # Too late!

domain = Rectangle(n0=100, n1=100)  # Error: MPI not initialized
\end{python}

This will produce an error:
\begin{verbatim}
RuntimeError: MPI has not been initialized. MPI must be
initialized before using escript domains. When using Python,
import mpi4py before importing escript, or use the run-escript
launcher for MPI programs.
\end{verbatim}

\subsubsection{Alternative: Use run-escript}

The \texttt{run-escript} launcher automatically handles \MPI initialization, so import order doesn't matter when using it:

\begin{verbatim}
run-escript myscript.py
\end{verbatim}

With \texttt{run-escript}, both of these work:
\begin{python}
# Both are fine when using run-escript
from esys.ripley import Rectangle
from mpi4py import MPI
\end{python}

However, for portability and clarity, it's still good practice to import \mpifo first.

\section{Basic Concepts}

\subsection{MPI Communicators}

An \MPI communicator defines a group of processes that can communicate with each other.
By default, all processes belong to \texttt{MPI\_COMM\_WORLD}, which includes every process in the job.

In \mpifo, you can access this default communicator:
\begin{python}
from mpi4py import MPI

comm = MPI.COMM_WORLD
rank = comm.Get_rank()  # My process rank (0 to size-1)
size = comm.Get_size()  # Total number of processes
\end{python}

\subsection{Splitting Communicators}

You can create smaller communicators by splitting \texttt{MPI\_COMM\_WORLD}:
\begin{python}
# Split into groups based on color
color = rank % 2  # Processes with same color go in same group
sub_comm = comm.Split(color, rank)

# Now sub_comm is a communicator with only part of the processes
sub_rank = sub_comm.Get_rank()
sub_size = sub_comm.Get_size()
\end{python}

Processes with the same \var{color} are grouped together into a new communicator.
The second argument to \texttt{Split()} determines the rank ordering within each new group.

\section{Passing Communicators to \escript Domains}

All \escript domain constructors accept an optional \var{comm} parameter that allows you to specify which \MPI communicator the domain should use:

\begin{python}
from mpi4py import MPI
from esys.ripley import Rectangle

# Create a domain using default MPI_COMM_WORLD
domain = Rectangle(n0=100, n1=100)

# Or explicitly pass MPI_COMM_WORLD
domain = Rectangle(n0=100, n1=100, comm=MPI.COMM_WORLD)

# Or use a custom communicator
sub_comm = MPI.COMM_WORLD.Split(color, rank)
domain = Rectangle(n0=100, n1=100, comm=sub_comm)
\end{python}

The \var{comm} parameter is available in all domain types:
\begin{itemize}
\item \textbf{Finley}: \texttt{ReadMesh()}, \texttt{ReadGmsh()}, \texttt{Rectangle()}, \texttt{Brick()}
\item \textbf{Ripley}: \texttt{Rectangle()}, \texttt{Brick()}, \texttt{MultiRectangle()}, \texttt{MultiBrick()}
\item \textbf{Speckley}: \texttt{Rectangle()}, \texttt{Brick()}
\item \textbf{Oxley}: \texttt{Rectangle()}, \texttt{Brick()} (requires Trilinos)
\end{itemize}

\section{Retrieving Communicators from \escript Objects}

You can retrieve the \MPI communicator from any \escript domain, function space, or data object:

\begin{python}
# From a domain
domain_comm = domain.getMPIComm()

# From a function space
fs = ContinuousFunction(domain)
fs_comm = fs.getMPIComm()

# From a data object
data = Scalar(1.0, fs)
data_comm = data.getMPIComm()

# All return the same communicator
assert MPI.Comm.Compare(domain_comm, fs_comm) == MPI.IDENT
assert MPI.Comm.Compare(fs_comm, data_comm) == MPI.IDENT
\end{python}

This allows you to:
\begin{itemize}
\item Verify which communicator a domain is using
\item Perform custom \MPI operations using the domain's communicator
\item Pass the communicator to other libraries or functions
\end{itemize}

\textbf{Important}: When comparing \MPI communicators, always use \texttt{MPI.Comm.Compare()}, never use \texttt{==}.
The comparison returns \texttt{MPI.IDENT} if the communicators are identical, \texttt{MPI.CONGRUENT} if they have the same processes but different contexts, or \texttt{MPI.UNEQUAL} if they differ.

\section{Simple Example: Ensemble Simulation}

This example demonstrates running multiple parameter values in parallel.
Each process group solves the same problem with a different parameter:

\begin{python}
from mpi4py import MPI
from esys.escript import *
from esys.escript.linearPDEs import LinearPDE
from esys.ripley import Rectangle

# Get MPI information
world_comm = MPI.COMM_WORLD
world_rank = world_comm.Get_rank()
world_size = world_comm.Get_size()

# Define parameter values to explore
parameters = [0.1, 0.5, 1.0, 2.0]
num_params = len(parameters)

# Ensure we have enough processes
if world_size < num_params:
    if world_rank == 0:
        print(f"Need at least {num_params} processes for this example")
    exit(1)

# Assign each process to a parameter
param_id = world_rank % num_params
my_parameter = parameters[param_id]

# Split communicator: processes with same parameter work together
processes_per_param = world_size // num_params
group_comm = world_comm.Split(param_id, world_rank)

# Create domain with group communicator
domain = Rectangle(n0=50, n1=50, comm=group_comm)

# Solve PDE with this parameter value
pde = LinearPDE(domain)
x = domain.getX()
pde.setValue(A=my_parameter*kronecker(2), Y=x[0]*x[1])
solution = pde.getSolution()

# Each group can now process its result independently
if group_comm.Get_rank() == 0:
    result = integrate(solution)
    print(f"Parameter {my_parameter}: Result = {result}")

# Clean up communicator
group_comm.Free()
\end{python}

\textbf{Running this example}:
\begin{verbatim}
mpirun -n 8 run-escript ensemble_example.py
\end{verbatim}

With 8 processes and 4 parameter values, each parameter gets 2 processes.
All 4 simulations run simultaneously.

\section{Advanced Example: Parameter Sweep}

This example shows a more realistic scenario where different grid resolutions are tested simultaneously:

\begin{python}
from mpi4py import MPI
from esys.escript import *
from esys.escript.linearPDEs import LinearPDE
from esys.ripley import Rectangle
import time

world_comm = MPI.COMM_WORLD
world_rank = world_comm.Get_rank()
world_size = world_comm.Get_size()

# Grid resolutions to test
grid_sizes = [
    (10, 10),   # Coarse
    (20, 20),   # Medium
    (40, 40),   # Fine
    (80, 80),   # Very fine
]
num_grids = len(grid_sizes)

# Split into groups
grid_id = world_rank % num_grids
nx, ny = grid_sizes[grid_id]
grid_comm = world_comm.Split(grid_id, world_rank)
grid_rank = grid_comm.Get_rank()

if world_rank == 0:
    print(f"Running {num_grids} grid resolutions simultaneously")
    print(f"Total processes: {world_size}")

# Synchronize before starting
world_comm.Barrier()
start_time = time.time()

# Create domain with this grid size
domain = Rectangle(n0=nx, n1=ny, comm=grid_comm)

# Solve a test problem
pde = LinearPDE(domain)
x = domain.getX()
pde.setValue(A=kronecker(2), Y=exp(-((x[0]-0.5)**2 + (x[1]-0.5)**2)/0.1))
solution = pde.getSolution()

# Compute result
result = integrate(solution)
elapsed = time.time() - start_time

# Leader of each group reports
if grid_rank == 0:
    procs = grid_comm.Get_size()
    print(f"Grid {nx}x{ny} ({procs} proc): "
          f"time={elapsed:.4f}s, result={result:.6f}")

# Gather all results to rank 0
all_results = world_comm.gather((nx, ny, elapsed, result), root=0)

if world_rank == 0:
    print("\nSummary:")
    print("  Grid Size  |  Time (s)  |   Result")
    print("-" * 42)
    for i in range(0, len(all_results), world_size // num_grids):
        nx, ny, t, r = all_results[i]
        print(f"  {nx:3d} x {ny:3d}   |   {t:6.4f}   |  {r:8.6f}")

grid_comm.Free()
\end{python}

\section{Master-Slave Pattern Example}

The master-slave pattern is useful when you have a collection of independent tasks to distribute among worker processes. One process (the master) manages task distribution and result collection, while other processes (slaves) compute results.

This example shows how to use a master-slave pattern to solve magnetotelluric (MT) problems for multiple frequencies:

\begin{python}
from mpi4py import MPI
from esys.escript import *
from esys.finley import Rectangle
from esys.escript.linearPDEs import LinearSinglePDE
import numpy as np

comm_world = MPI.COMM_WORLD
rank = comm_world.Get_rank()
size = comm_world.Get_size()

def solve_MT_for_frequency(freq):
    """Solve MT problem using MPI.COMM_SELF (single process)"""
    # Create independent domain
    domain = Rectangle(n0=200, n1=100, l0=40000, l1=20000,
                      comm=MPI.COMM_SELF)

    # Setup PDE (complex-valued for frequency domain)
    pde = LinearSinglePDE(domain, isComplex=True)
    Mu0 = 4*np.pi*1e-7
    pde.setValue(D=1j*2*np.pi*freq*Mu0)

    # Set resistivity and boundary conditions
    # (simplified - see full example for details)
    pde.setValue(A=100*kronecker(2))  # 100 Ohm-m
    x = domain.getX()
    pde.setValue(q=whereZero(x[1]-20000), r=1)

    # Solve and return impedance
    Hx = pde.getSolution()
    return integrate(abs(Hx))

if rank == 0:
    # MASTER PROCESS
    frequencies = np.logspace(-1, 1, 10)  # 0.1 to 10 Hz
    results = []
    tasks_sent = 0
    tasks_received = 0

    # Send initial tasks to slaves
    for slave in range(1, min(size, len(frequencies)+1)):
        if tasks_sent < len(frequencies):
            comm_world.send(frequencies[tasks_sent], dest=slave, tag=1)
            tasks_sent += 1

    # Receive results and send more tasks
    while tasks_received < len(frequencies):
        # Receive result from any slave
        status = MPI.Status()
        result = comm_world.recv(source=MPI.ANY_SOURCE, tag=2,
                                status=status)
        slave = status.Get_source()
        results.append(result)
        tasks_received += 1

        # Send next task if available
        if tasks_sent < len(frequencies):
            comm_world.send(frequencies[tasks_sent], dest=slave, tag=1)
            tasks_sent += 1

    # Send termination signal
    for slave in range(1, size):
        comm_world.send(None, dest=slave, tag=1)

    print(f"Completed {len(results)} frequencies")

else:
    # SLAVE PROCESS
    while True:
        # Receive task from master
        freq = comm_world.recv(source=0, tag=1)

        # Check for termination
        if freq is None:
            break

        # Solve problem
        result = solve_MT_for_frequency(freq)

        # Send result back
        comm_world.send((freq, result), dest=0, tag=2)

comm_world.Barrier()
\end{python}

\textbf{Key features of this pattern}:
\begin{itemize}
\item \textbf{Dynamic load balancing}: Master assigns tasks as slaves become available
\item \textbf{Independent domains}: Each slave uses \texttt{MPI.COMM\_SELF} for its own domain
\item \textbf{Asynchronous communication}: \texttt{MPI.ANY\_SOURCE} allows receiving from any slave
\item \textbf{Scalability}: Works with any number of slave processes
\end{itemize}

\textbf{Running this example}:
\begin{verbatim}
mpirun -n 11 run-escript mt_master_slave.py
\end{verbatim}

This runs with 1 master and 10 slaves, solving 10 frequencies efficiently. If more slaves than tasks are available, some slaves will be idle.

A complete working example (\texttt{MT\_master\_slave.py}) is provided in the \texttt{doc/examples/usersguide/} directory.

\section{Multi-Physics Example: Thermo-Mechanical Coupling}

This example demonstrates time-dependent coupling of thermal and mechanical physics on separate communicators. The thermal solver computes temperature evolution while the mechanical solver calculates thermal stresses. The mechanical model runs one time-step behind to use the temperature from the previous step (staggered coupling).

\begin{python}
from mpi4py import MPI
from esys.escript import *
from esys.escript.linearPDEs import LinearPDE
from esys.ripley import Rectangle
import numpy as np

world_comm = MPI.COMM_WORLD
world_rank = world_comm.Get_rank()
world_size = world_comm.Get_size()

# Split processes: first half for thermal, second half for mechanical
is_thermal = (world_rank < world_size // 2)
physics_id = 0 if is_thermal else 1

# Create separate communicators for each physics
physics_comm = world_comm.Split(physics_id, world_rank)
physics_rank = physics_comm.Get_rank()

# Create domain on physics-specific communicator
domain = Rectangle(n0=80, n1=80, l0=1.0, l1=1.0, comm=physics_comm)
x = domain.getX()

# Time stepping parameters
dt = 0.01      # Time step
n_steps = 50   # Number of time steps

if is_thermal:
    # THERMAL SOLVER: Transient heat equation
    # du/dt = kappa * nabla^2(u) + Q
    from esys.escript.linearPDEs import LinearPDE

    pde = LinearPDE(domain)
    kappa = 1.0  # Thermal diffusivity
    pde.setValue(A=kappa*kronecker(2), D=1.0)

    # Initial temperature: hot spot in center
    T = 20.0 + 100.0*exp(-50*((x[0]-0.5)**2 + (x[1]-0.5)**2))
    T_old = T

    for step in range(n_steps):
        # Heat source (decaying with time)
        Q = 10.0 * exp(-0.1*step*dt) * \
            exp(-50*((x[0]-0.5)**2 + (x[1]-0.5)**2))

        # Backward Euler: (T - T_old)/dt = kappa*nabla^2(T) + Q
        pde.setValue(Y=T_old/dt + Q)
        T = pde.getSolution()

        # Send temperature field to mechanical solver
        if physics_rank == 0:
            # Convert to numpy for efficient MPI transfer
            T_data = interpolate(T, Function(domain))
            T_array = convertToNumpy(T_data)
            world_comm.send(T_array, dest=world_size//2, tag=step)

            if step % 10 == 0:
                T_max = Lsup(T)
                print(f"Thermal step {step}: T_max = {T_max:.2f}")

        T_old = T

else:
    # MECHANICAL SOLVER: Thermal stress (one step behind)
    # Solve: -div(sigma) = 0, sigma = E*epsilon - alpha*E*Delta_T
    from esys.escript.linearPDEs import LinearPDE

    pde = LinearPDE(domain, numEquations=2)
    E = 70e9        # Young's modulus (Pa)
    nu = 0.3        # Poisson's ratio
    alpha = 23e-6   # Thermal expansion coefficient (1/K)
    T0 = 20.0       # Reference temperature

    # Elastic stiffness tensor (plane stress)
    C = E/(1-nu**2) * kronecker(2)
    pde.setValue(A=C)

    # Fixed displacement on left edge
    x = domain.getX()
    pde.setValue(q=whereZero(x[0]), r=[0.0, 0.0])

    for step in range(1, n_steps):  # Start at 1 (one step behind)
        # Receive temperature from thermal solver (previous step)
        if physics_rank == 0:
            T_array = world_comm.recv(source=0, tag=step-1)
            # Reconstruct escript Data from numpy array
            T_data = Data(T_array, Function(domain))
        else:
            T_data = None

        # Broadcast temperature within mechanical group
        T_data = physics_comm.bcast(T_data, root=0)

        # Thermal strain causes body force
        Delta_T = T_data - T0
        thermal_stress = alpha * E/(1-nu) * Delta_T

        # Thermal stress acts as body force in equilibrium equation
        pde.setValue(Y=[grad(thermal_stress)[0], grad(thermal_stress)[1]])

        # Solve for displacement
        u = pde.getSolution()

        if physics_rank == 0 and step % 10 == 0:
            u_max = Lsup(length(u))
            print(f"Mechanical step {step}: u_max = {u_max:.2e} m")

physics_comm.Free()
world_comm.Barrier()
\end{python}

\textbf{Key features of this coupled simulation}:
\begin{itemize}
\item \textbf{Staggered coupling}: Mechanical solver uses temperature from the previous time step
\item \textbf{Parallel physics}: Both solvers run simultaneously on separate process groups
\item \textbf{Data exchange}: Temperature field transferred between physics at each step
\item \textbf{Realistic model}: Thermal expansion causes mechanical deformation
\item \textbf{Scalability}: Each physics can use different number of processes
\end{itemize}

\textbf{Running this example}:
\begin{verbatim}
mpirun -n 8 run-escript thermo_mechanical.py
\end{verbatim}

This uses 4 processes for thermal and 4 for mechanical. The thermal solver is always one step ahead, computing the temperature field that the mechanical solver will use in the next iteration.

\section{Master-Slave Pattern for Embarrassingly Parallel Problems}

The master-slave pattern is ideal for \textbf{embarrassingly parallel} problems where many independent computations must be performed with different parameters. One master process distributes tasks to multiple slave processes, which compute results independently and return them to the master.

This example demonstrates solving a magnetotelluric (MT) problem for multiple frequencies in parallel. The MT method uses electromagnetic induction to image subsurface electrical resistivity. Each frequency requires solving a complex-valued PDE, but the solutions are independent, making this an ideal master-slave problem.

\subsection{Domain Creation Strategy}

Each slave process creates its own domain on \texttt{MPI.COMM\_SELF} (a communicator containing only that process). This allows:
\begin{itemize}
\item \textbf{Independent execution}: Each slave solves its problem without MPI collective operations
\item \textbf{OpenMP parallelization}: Each domain can use OpenMP threads for shared-memory parallelism
\item \textbf{Efficiency}: Domain is created once and reused for all frequencies assigned to that slave
\end{itemize}

\textbf{Key code structure} from \texttt{MT\_master\_slave.py}:

\begin{python}
from mpi4py import MPI
from esys.escript import *
from esys.finley import Rectangle

comm_world = MPI.COMM_WORLD
rank = comm_world.Get_rank()
size = comm_world.Get_size()

# Create domain once on COMM_SELF (single MPI rank with OpenMP)
# Each process creates its own independent domain
domain = Rectangle(n0=NE0, n1=NE1, l0=L0, l1=L1, comm=MPI.COMM_SELF)

def solve_MT_problem(frequency):
    """Solve MT problem for given frequency using pre-created domain."""

    # Set up complex-valued PDE for magnetic field
    pde = LinearSinglePDE(domain, isComplex=True)
    pde.setValue(D=1j*2*np.pi*frequency*Mu0)

    # Define resistivity distribution (anomaly + background)
    rho = rho_b * (1-m) + rho_a * m  # m is anomaly mask
    pde.setValue(A=rho*np.eye(2))

    # Boundary condition: Hx = 1 at top surface
    pde.setValue(q=mD, r=1)

    # Solve for magnetic field
    Hx = pde.getSolution()

    # Calculate impedance and apparent resistivity
    Ey = rho * grad(Hx)[1]
    Zyx = Ey / Hx
    rho_a = 1./(2*np.pi*frequency*Mu0) * abs(Zyx)**2

    return x_positions, rho_a, phase

def master_process():
    """Distribute frequencies to slaves and collect results."""
    frequencies = np.logspace(-1, 1, 10)  # 0.1 to 10 Hz

    # Send initial tasks to all slaves
    for slave_rank in range(1, size):
        if tasks_sent < len(frequencies):
            comm_world.send(frequencies[tasks_sent], dest=slave_rank, tag=1)
            tasks_sent += 1

    # Receive results and send new tasks
    while tasks_received < len(frequencies):
        result = comm_world.recv(source=MPI.ANY_SOURCE, tag=2, status=status)
        results.append(result)

        # Send next task if available
        if tasks_sent < len(frequencies):
            comm_world.send(frequencies[tasks_sent], dest=slave_rank, tag=1)

    # Send termination signal
    for slave_rank in range(1, size):
        comm_world.send(None, dest=slave_rank, tag=1)

    plot_results(results)

def slave_process():
    """Receive frequencies, solve, and return results."""
    while True:
        freq = comm_world.recv(source=0, tag=1)
        if freq is None:  # Termination signal
            break

        # Solve MT problem for this frequency
        x_ts, rho_a_ts, phi_ts = solve_MT_problem(freq)

        # Send results back to master
        comm_world.send((freq, x_ts, rho_a_ts, phi_ts), dest=0, tag=2)

# Main execution
if rank == 0:
    master_process()
else:
    slave_process()
\end{python}

\subsection{Performance Considerations}

For embarrassingly parallel problems with relatively small domains:
\begin{verbatim}
OMP_NUM_THREADS=1 mpirun -n 11 python3 MT_master_slave.py
\end{verbatim}

Setting \texttt{OMP\_NUM\_THREADS=1} maximizes parallelism by using more slave processes rather than OpenMP threads. With 11 processes (1 master + 10 slaves), you can solve 10 frequencies simultaneously.

\subsection{Results}

Figure~\ref{fig:mt_master_slave} shows apparent resistivity and phase computed for 10 frequencies (0.1--10~Hz). The conductive anomaly (0.5~$\Omega$m) within the resistive background (100~$\Omega$m) creates a characteristic signature that varies with frequency due to electromagnetic skin depth effects.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/MT_master_slave_results.png}
\caption{Magnetotelluric apparent resistivity and phase transects computed for 10 frequencies using the master-slave pattern. Lower frequencies penetrate deeper and show stronger anomaly response.}
\label{fig:mt_master_slave}
\end{figure}

Figure~\ref{fig:mt_normalized} shows the normalized resistivity (apparent resistivity divided by background resistivity), clearly revealing the frequency-dependent anomaly signature centered at 41~km.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/MT_master_slave_normalized.png}
\caption{Normalized resistivity showing frequency-dependent anomaly response. The anomaly signature is strongest at lower frequencies which penetrate deeper into the subsurface.}
\label{fig:mt_normalized}
\end{figure}

\textbf{Key advantages of the master-slave pattern}:
\begin{itemize}
\item \textbf{Load balancing}: Master dynamically assigns work as slaves finish
\item \textbf{Scalability}: Add more slaves to process more parameters in parallel
\item \textbf{Simplicity}: No complex inter-process communication or synchronization
\item \textbf{Fault tolerance}: Easy to detect and reassign failed tasks
\item \textbf{Resource efficiency}: Each slave uses minimal MPI resources (single rank)
\end{itemize}

The complete example code is available in \texttt{doc/examples/usersguide/MT\_master\_slave.py}.

\section{Best Practices}

\subsection{Communicator Management}

Always free custom communicators when you're done with them:
\begin{python}
sub_comm = MPI.COMM_WORLD.Split(color, rank)
# ... use sub_comm ...
sub_comm.Free()  # Important: free when done
\end{python}

Do \textbf{not} free \texttt{MPI.COMM\_WORLD} or communicators returned by \texttt{getMPIComm()}, as these are managed by the system.

\subsection{Synchronization}

Use barriers when you need to synchronize across groups:
\begin{python}
# Synchronize within a group
group_comm.Barrier()

# Synchronize all processes
MPI.COMM_WORLD.Barrier()
\end{python}

\subsection{Error Handling}

Exceptions in one process group won't automatically propagate to others.
Consider using \MPI collective operations to detect and handle errors:

\begin{python}
try:
    # Solve PDE
    solution = pde.getSolution()
    error_code = 0
except Exception as e:
    print(f"Rank {rank} error: {e}")
    error_code = 1

# Check if any process had an error
global_error = group_comm.allreduce(error_code, op=MPI.MAX)
if global_error != 0:
    print(f"Group {group_id} had errors")
\end{python}

\subsection{Load Balancing}

When splitting into groups, ensure each group has sufficient resources:
\begin{python}
# Good: evenly divides processes
num_groups = 4
group_id = rank // (world_size // num_groups)

# Bad: might create groups with different sizes
group_id = rank % num_groups  # Only works if size divisible by num_groups
\end{python}

\section{Comparison with SplitWorld}

\escript's \texttt{SplitWorld} class (Chapter~\ref{CHAP:subworld}) provides similar functionality but with a higher-level interface.

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Feature} & \textbf{mpi4py} & \textbf{SplitWorld} \\
\hline
Flexibility & Full \MPI control & Predefined patterns \\
Learning curve & Requires \MPI knowledge & Easier for beginners \\
Communicator control & Direct access & Abstracted \\
Custom communication & Easy & Limited \\
Standard MPI & Yes & No \\
Interoperability & Works with other libraries & \escript-specific \\
\hline
\end{tabular}
\end{center}

\textbf{When to use mpi4py}:
\begin{itemize}
\item You need fine-grained control over process organization
\item You want to integrate with other \MPI-based libraries
\item You're familiar with \MPI programming
\item You need custom communication patterns
\end{itemize}

\textbf{When to use SplitWorld}:
\begin{itemize}
\item You want a simple, high-level interface
\item You're running straightforward ensemble simulations
\item You don't need custom \MPI operations
\end{itemize}

\section{Performance Considerations}

\subsection{Process Group Size}

Each domain needs enough processes to solve efficiently:
\begin{itemize}
\item Very small groups (1-2 processes) may not parallelize well
\item Very large groups waste resources on small problems
\item Experiment to find the optimal group size for your problem
\end{itemize}

\subsection{Communication Overhead}

Minimize communication between process groups:
\begin{itemize}
\item Use group-local operations when possible
\item Reduce data before sending between groups
\item Consider asynchronous communication (\texttt{isend/irecv})
\end{itemize}

\subsection{Load Balancing}

Ensure all groups have similar workloads:
\begin{itemize}
\item Groups with different problem sizes finish at different times
\item The slowest group determines overall runtime
\item Use dynamic work distribution for highly variable problems
\end{itemize}

\section{Advanced Topics}

\subsection{Custom Topologies}

Create communicators with specific topologies:
\begin{python}
# Cartesian topology for structured grids
dims = [2, 2]  # 2x2 process grid
periods = [False, False]  # Non-periodic boundaries
cart_comm = comm.Create_cart(dims, periods=periods)

# Use in escript domain
domain = Rectangle(n0=100, n1=100, comm=cart_comm)
\end{python}

\subsection{Interoperability}

The communicators work with other \MPI-based Python libraries:
\begin{python}
# Use with mpi4py
comm = domain.getMPIComm()

# Send data using standard MPI
if comm.Get_rank() == 0:
    data = numpy.array([1.0, 2.0, 3.0])
    comm.Send(data, dest=1, tag=11)
\end{python}

\subsection{Fault Tolerance}

Handle communicator-specific failures:
\begin{python}
try:
    solution = pde.getSolution()
    success = 1
except:
    success = 0

# Check if all processes succeeded
all_success = group_comm.allreduce(success, op=MPI.MIN)
if all_success == 0:
    # At least one process failed
    if group_rank == 0:
        print(f"Group {group_id} failed")
\end{python}

\section{Summary}

Using \mpifo with \escript provides:
\begin{itemize}
\item Direct control over parallel resource allocation
\item Standard \MPI programming techniques
\item Flexibility for complex parallel patterns
\item Interoperability with other \MPI libraries
\item Fine-grained performance tuning
\end{itemize}

Key capabilities:
\begin{itemize}
\item Pass custom communicators to domains via \var{comm} parameter
\item Retrieve communicators via \texttt{getMPIComm()}
\item Run ensemble simulations and parameter sweeps
\item Implement multi-physics coupling
\item Use standard \MPI collective operations
\end{itemize}

For more information:
\begin{itemize}
\item \mpifo documentation: \url{https://mpi4py.readthedocs.io/}
\item \MPI standard: \url{https://www.mpi-forum.org/}
\item \escript examples directory: \texttt{doc/examples/}
\end{itemize}
